\documentclass[bachelor, och, coursework]{SCWorks}
% параметр - тип обучения - одно из значений:
%    spec     - специальность
%    bachelor - бакалавриат (по умолчанию)
%    master   - магистратура
% параметр - форма обучения - одно из значений:
%    och   - очное (по умолчанию)
%    zaoch - заочное
% параметр - тип работы - одно из значений:
%    referat    - реферат
%    coursework - курсовая работа (по умолчанию)
%    diploma    - дипломная работа
%    pract      - отчет по практике
%    pract      - отчет о научно-исследовательской работе
%    autoref    - автореферат выпускной работы
%    assignment - задание на выпускную квалификационную работу
%    review     - отзыв руководителя
%    critique   - рецензия на выпускную работу
% параметр - включение шрифта
%    times    - включение шрифта Times New Roman (если установлен)
%               по умолчанию выключен
\usepackage[T2A]{fontenc}
\usepackage[cp1251]{inputenc}
\usepackage{graphicx}

\usepackage[sort,compress]{cite}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{fancyvrb}
\usepackage{longtable}
\usepackage{array}
\usepackage[english,russian]{babel}
\usepackage{tempora}
\usepackage{minted}
\usepackage{cancel}

\usepackage[colorlinks=true]{hyperref}


\newcommand{\eqdef}{\stackrel {\rm def}{=}}

\newtheorem{lem}{Лемма}

\begin{document}

% Кафедра (в родительном падеже)
\chair{математической кибернетики и компьютерных наук}

% Тема работы
\title{Тематическое моделирование новостей}

% Курс
\course{3}

% Группа
\group{351}

% Факультет (в родительном падеже) (по умолчанию "факультета КНиИТ")
%\department{факультета КНиИТ}

% Специальность/направление код - наименование
%\napravlenie{02.03.02 "--- Фундаментальная информатика и информационные технологии}
%\napravlenie{02.03.01 "--- Математическое обеспечение и администрирование информационных систем}
%\napravlenie{09.03.01 "--- Информатика и вычислительная техника}
\napravlenie{09.03.04 "--- Программная инженерия}
%\napravlenie{10.05.01 "--- Компьютерная безопасность}

% Для студентки. Для работы студента следующая команда не нужна.
%\studenttitle{Студентки}

% Фамилия, имя, отчество в родительном падеже
\author{Кондрашова Даниила Владиславовича}

% Заведующий кафедрой
\chtitle{к.\,ф.-м.\,н.} % степень, звание
\chname{С.\,В.\,Миронов}

%Научный руководитель (для реферата преподаватель проверяющий работу)
\satitle{доцент, к.\,ф.-м.\,н.} %должность, степень, звание
\saname{С.\,В.\,Папшев}

% Руководитель практики от организации (только для практики,
% для остальных типов работ не используется)
%\patitle{к.\,ф.-м.\,н., доцент}
%\paname{Д.\,Ю.\,Петров}

% Семестр (только для практики, для остальных
% типов работ не используется)
%\term{2}

% Наименование практики (только для практики, для остальных
% типов работ не используется)
%\practtype{учебная}

% Продолжительность практики (количество недель) (только для практики,
% для остальных типов работ не используется)
%\duration{2}

% Даты начала и окончания практики (только для практики, для остальных
% типов работ не используется)
%\practStart{01.07.2016}
%\practFinish{14.07.2016}

% Год выполнения отчета
\date{2024}

\maketitle

% Включение нумерации рисунков, формул и таблиц по разделам
% (по умолчанию - нумерация сквозная)
% (допускается оба вида нумерации)
%\secNumbering


\tableofcontents

% Раздел "Обозначения и сокращения". Может отсутствовать в работе
%\abbreviations

% Раздел "Определения". Может отсутствовать в работе
%\definitions

% Раздел "Определения, обозначения и сокращения". Может отсутствовать в работе.
% Если присутствует, то заменяет собой разделы "Обозначения и сокращения" и "Определения"
%\defabbr


% Раздел "Введение"
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\intro
С ростом объёмов информации в современном мире умение классифицировать и
структурировать данные становится необходимым для их эффективного поиска и
изучения. Физически невозможно найти нужные сведения, просто перебирая все
ресурсы подряд, поэтому возникает острая потребность в тематическом поиске и
классификации данных.

Тематическое моделирование призвано решить эту проблему. Оно позволяет быстро и
эффективно автоматически разбивать большие объёмы информации по темам, упрощая
процесс поиска и анализа данных.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Математические основы тематического моделирования}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Основная гипотеза тематического моделирования}
Тематическое моделирование "--- это метод анализа текстовых данных, который
позволяет выявлять семантические структуры в коллекциях документов.

Основная идея тематического моделирования заключается в том, что слова в тексте
связаны не с конкретным документом, а с темами. Сначала текст разбивается на
темы, и каждая из них генерирует слова для соответствующих позиций в документе.
Таким образом, сначала формируется тема, а замет тема формирует терм.

Эта гипотеза позволяет проводить тематическую классификацию текстов на основе
частоты и взаимовстречаемости слов~\cite{all, pdflec1, videolec1}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Аксиоматика тематического моделирования}
Каждый текст можно количественно охарактеризовать. Вот основные количественные
характеристики, использующиеся при тематическом моделировании:
\begin{itemize}
    \item $W$ "--- конечное множество термов;
    \item $D$ "--- конечное множество текстовых документов;
    \item $T$ "--- конечное множество тем;
    \item $D \times W \times T$ "--- дискретное вероятностное пространство;
    \item коллекция "--- i.i.d выборка $(d_i, w_i, t_i)^n_{i = 1}$;
    \item $n_{dwt} = \sum^n_{i = 1} [d_i = d][w_i = w][t_i = t]$ "--- частота
    $(d, w, t)$ в коллекции;
    \item $n_{wt} = \sum_d n_{dwt}$ "--- частота терма $w$ в документе $d$;
    \item $n_{td} = \sum_w n_{dwt}$ "--- частота термов темы $t$ в документе
    $d$;
    \item $n_t = \sum_{d, w} n_{dwt}$ "--- частота термов темы $t$ в коллекции;
    \item $n_{dw} = \sum_t n_{dwt}$ "--- частота терма $w$ в документе $d$;
    \item $n_W = \sum_d n_{dw}$ "--- частота терма $w$ в коллекции;
    \item $n_d = \sum_w n_{dw}$ "--- длина документа $d$;
    \item $n = \sum_{d, w} n_{dw}$ "--- длина коллекции.
\end{itemize}

Также в тематическом моделировании используются следующие гипотезы и аксиомы:
\begin{itemize}
    \item Независимость слов от порядка в документе: порядок слов в документе
    не важен;
    \item Независимость от порядка документов в коллекции: порядок документов
    в коллекции не важен;
    \item Зависимость терма от темы: каждый терм связан с соответствующей темой
    и порождается ей;
    \item Гипотеза условной независимости: $p(w|d, t) = p(w|t)$.
\end{itemize}

Вышеперечисленные характеристики, гипотезы и аксиомы составляют основу
тематического моделирования и являются достаточными для построения
тематической модели.~\cite{teorver, all, pdflec1, videolec1}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Задача тематического моделирования}
Как уже говорилось ранее, документ порождается следующим образом:
\begin{enumerate}
    \item для каждой позиции в документе генерируется тема $p(t|d)$;
    \item для каждой сгенерированной темы в соответствующей позиции генерируем
    терм $p(w|d, t)$.
\end{enumerate}

\begin{figure}[!ht]
	\centering
	\includegraphics[width=16cm]{./images/2.png}
	\caption{\label{fig:2}%
	Алгоритм формирования документа}
\end{figure}

Тогда вероятность появления слова в документе можно описать по формуле полной
вероятности:
\begin{equation}
    p(w|d) = \sum_{t \in T} p(w|d,t)p(t|d) = \sum_{t \in T} p(w|t)p(t|d) 
\end{equation}

Такой алгоритм является прямой задачей порождения текста. Тематическое
моделирование призвано решить обратную задачу:
\begin{enumerate}
    \item для каждого терма $w$ в тексте найти вероятность появления в теме $t$
    (найти $p(w|t) = \phi_{wt}$);
    \item для каждой темы $t$ найти вероятность появления в документе $d$
    (найти $p(t|d) = \theta_{td}$).
\end{enumerate}

Обратную задачу можно представить в виде стохастического матричного
разложения~\ref{fig:1}.

\begin{figure}[!ht]
	\centering
	\includegraphics[width=16cm]{./images/1.png}
	\caption{\label{fig:1}%
	Стохастическое матричное разложение}
\end{figure}


Таким образом, тематическое моделирование ищет величину
$p(w|d)$~\cite{teorver, all, pdflec1, videolec1}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Решение обратной задачи}
Для решения задачи тематического моделирования необходимо найти величину
$p(w|d)$, сделать это можно с помощью метода максимального правдоподобия.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Лемма о максимизации функции на единичных симплексах} \label{sec:01}
Перед тем как перейти к решению обратной задачи, сформулируем лемму,
которая поможет нам в этом процессе.

Введём операцию нормировки вектора:
\begin{equation}
    p_i = \underset{i \in I}(x_i) =
    \frac{\max{x_i, 0}}{\sum_{k \in I} \max{x_k, 0}}
\end{equation}

\textbf{Лемма о максимизации функции на единичных симплексах:}

Пусть функция $f(\Omega)$ непрерывно дифференцируема по набору векторов
$\Omega = (w_i)_{j \in J}, \;\; w_j = (w_{ij})_{i \in I_j}$ различных
размерностей $|I_j|$. Тогда векторы $w_j$ локального экстремума задачи
\begin{equation*}
    \begin{cases}
        f(\Omega) \to \underset{\Omega}{\max} \\
        \sum_{i \in I_j} w_{ij} = 1, \;\; j \in J \\
        w_{ij} \geq 0, \;\; i \in I_j, j \in J
    \end{cases}
\end{equation*}

при условии $1^0: \;\; (\exists i \in I_j) w_{ij}
\frac{\partial f}{\partial w_{ij}} > 0$ удовлетворяют уравнениям
\begin{equation}
    w_{ij} = \underset{i \in I_j}{norm}\left(w_{ij}
    \frac{\partial f}{\partial w_{ij}}\right), \;\; i \in I_j;
\end{equation}

при условии $2^0: \;\; (\forall i \in I_j) w_{ij}
\frac{\partial f}{\partial w_{ij}} \leq 0$ и $(\exists i \in I_j) w_{ij}
\frac{\partial f}{\partial w_{ij}} < 0$ удовлетворяют уравнениям
\begin{equation}
    w_{ij} = \underset{i \in I_j}{norm}\left(-w_{ij}
    \frac{\partial f}{\partial w_{ij}}\right), \;\; i \in I_j;
\end{equation}

в противном случае (условие $3^0$) "--- однородным уравнениям
\begin{equation}
    w_{ij}\frac{\partial f}{\partial w_{ij}} = 0, \;\; i \in I_j.
\end{equation}

Данная лемма служит для оптимизации любых моделей, параметрами которых являются
неотрицательные нормированные векторы~\cite{all, pdflec1, videolec1}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Сведение обратной задачи к задаче максимизации функционала}
Чтобы вычислить величину $p(w|d)$ воспользуемся принципом максимума
правдоподобия, согласно которому будут подобраны параметры $\Phi, \Theta$
такие, что $p(w|d)$ примет наибольшее значение.

\begin{equation}
    \prod^n_{i = 1} p(d_i, w_i) = \prod_{d \in D} \prod_{w \in d} p(d, w)^{
        n_{dw}}
\end{equation}

Прологарифмировав правдоподобие, перейдём к задаче максимизации логарифма 
правдоподобия.
\begin{equation}
    \sum_{d \in D} \sum_{w \in d} n_{dw} \ln{p(w|d)
    \underset{const}{\xcancel{p(d)}}} = n_{dw} \to max
\end{equation}

Данная задача эквивалентна задаче максимизации функционала
\begin{equation} \label{eq:01}
    L(\Phi, \Theta) = \sum_{d \in D} \sum_{w \in d} n_{dw} \ln{\sum_{t \in T}
    \phi_{wt} \theta_{td}} \to \underset{\Phi, \Theta}{max}
\end{equation}

при ограничениях неотрицательности и нормировки
\begin{equation} \label{eq:02}
    \phi_{wt} \geq 0; \;\; \sum_{w \in W} \phi_{wt} = 1; \;\;\; \theta_{td}
	\geq 0; \;\; \sum_{t \in T} \theta_{td} = 1
\end{equation}

Таким образом, обратная задача сводится к задаче максимизации
функции~\cite{teorver, all, pdflec1, videolec1}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Аддитивная регуляризация тематических моделей}
Задача~\cite{eq:01} не соответствует критериям корректно поставленной задачи
по Адамару, поскольку в общем случае она имеет бесконечное множество решений.
Это свидетельствует о необходимости доопределения задачи. 

Для доопределения некорректно поставленных задач применяется регуляризация: к
основному критерию добавляется дополнительный критерий "--- регуляризатор,
который соответствует специфике решаемой задачи. 

Метод ARTM (аддитивная регуляризация тематических моделей) основывается на
максимизации линейной комбинации логарифма правдоподобия и регуляризаторов
$R_i(\Phi, \Theta)$ с неотрицательными коэффициентами регуляризации $t\tau_i, \;
\; i = 1, \dots, k$.

Преобразуем задачу к ARTM виду:
\begin{equation}
    \sum_{d \in D} \sum_{w \in d} n_{dw} \ln{\sum_{t \in T}
    \phi_{wt} \theta_{td}} + R(\Phi, \Theta) \to \underset{\Phi, \Theta}{max};
    \;\; R(\Phi, \Theta) = \sum^k_{i = 1} \tau_i R_i(\Phi, \Theta)
\end{equation}

при ограничениях неотрицательности и нормировки~\ref{eq:02}.

Регуляризатор (или набор регуляризаторов) выбирается в соответствии с решаемой
задачей~\cite{all, pdflec1, videolec1}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{E"=M алгоритм}
Из представленных ограничений~\ref{eq:02} следует, что столбцы матриц можно
считать неотрицательными единичными векторами. Таким образом, задача сводится к
максимизации функции на единичных симплексах.

Воспользуемся леммой о максимизации функции на единичных
симплексах~\ref{sec:01} и перепишем задачу.

Пусть функция $R(\Phi, \Theta)$ непрерывно дифференцируема. Тогда точка $(\Phi,
\Theta)$ локального экстремума задачи с ограничениями, удовлетворяет системе
уравнений с вспомогательными переменными $p_{twd} = p(t|d, w)$, если из
решения исключить нулевые столбцы матриц $\Phi$ и $\Theta$:

\begin{equation} \label{eq:03}
    \begin{cases}
        p_{tdw} = \underset{t \in T}{norm}(\phi_{wt}\theta_{td}) \\
        \phi_{wt} = \underset{w \in W}{norm}\left(n_{wt} + \phi_{wt}
        \frac{\partial R}{\partial \phi_{wt}}\right); \\
        \theta_{td} = \underset{t \in T}{norm}\left(n_{td} + \theta_{td}
        \frac{\partial R}{\partial \theta_{td}}\right)
    \end{cases}
\end{equation}

Полученная модель соответствует E"=M алгоритму, где первая строка системы
уравнений соответствует E шагу, а вторая и третья строки "--- M шагу.

Решив полученную систему уравнений, методом простых итерации получим искомые
матрицы $\Phi$ и $\Theta$~\cite{all, pdflec1, videolec1}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Регуляризаторы в тематическом моделировании}
В этом разделе будут рассмотрены некоторые возможные варианты регуляризаторов.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Дивергенция Кульбака"=Лейблера} \label{sec:02}
Перед тем как перейти к регуляризаторам необходимо ввести меру оценки близости
тем.

Чтобы оценить близость тем можно воспользователься дивергенцией
Кульбака"=Лейблера (KL или KL"=дивергенция). KL"=дивергенция позволяет оценить
степень вложенности одного распределения в другое, в случае тематического
моделирования будет оценитьваться вложенность матриц.

Определим KL"=дивергенцию:

Пусть $P = (p_i)^n_{i = 1}$ и $Q = (q_i)^n_{i = 1}$ некоторые распределения.
Тогда дивергенция Кульбака"=Лейблера имеет следующий вид:
\begin{equation}
    KL(P||Q) = KL_i(p_i||q_i) = \sum^n_{i = 1} p_i \ln{\frac{p_i}{q_i}}.
\end{equation}

Свойства KL"=дивергенции:
\begin{enumerate}
    \item $KL(P||Q) \geq 0$;
    \item $KL(P||Q) = 0 \;\; \Leftrightarrow \;\; P = Q$;
    \item Минимизация KL эквивалентна максимизации правдоподобия:
    $$KL(P||Q(\alpha)) = \sum^n_{i = 1} p_i \ln{\frac{p_i}{q_i(\alpha)}} \to
    \underset{\alpha}{\min} \;\; \Leftrightarrow \;\; \sum^n_{i = 1} p_i
    \ln{q_i}(\alpha) \to \underset{\alpha}{\max};$$ \label{it:kl3}
    \item Если $KL(P||Q) < KL(Q||P)$, то $P$ сильнее вложено в $Q$, чем $Q$ в
    $P$.
\end{enumerate}

Теперь можно перейти к рассмотрению
регуляризаторов~\cite{all, pdflec2, videolec2}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Регуляризатор сглаживания}
Сглаживание предполагает сематническое сближение тем, это может быть полезно в
следующих случаях:
\begin{enumerate}
    \item Темы могут быть похожи между собой по терминологии, например,
    основы теории вероятностей и линейной алгебры обладают рядом одинаковых
    терминов;
    \item При выделении фоновых тем важно максимально вобрать в них слова,
    следовательно, сглаживание поможет решить эту задачу.
\end{enumerate}

Определим регуляризатор сглаживания:

Пусть распределения $\phi_{wt}$ близки к заданному распределению $\beta_w$
и пусть распределения $\theta_{td}$ близки к заданному распределению $\alpha_t$.
Тогда в форме KL"=дивергеннции~\ref{sec:02} выразим задачу сглаживания:
\begin{equation}
    \sum_{t \in T} KL(\beta_w||\phi_{wt}) \to \underset{\Phi}{\min}; \;\;
    \sum_{d \in D} KL(\alpha_t||\theta_{td}) \to \underset{\Theta}{\min}.
\end{equation}

Согласно свойству~\ref{it:kl3} KL"=дивергенции перейдём к задаче максимизации
правдоподобия:
\begin{equation}
    R(\Phi, \Theta) = \beta_o \sum_{t \in T}\sum_{w \in W} \beta_w
	\ln{\phi_{wt}} + \alpha_0 \sum_{d \in D}\sum_{t \in T}\alpha_t
	\ln{\theta_{td}} \to \max.
\end{equation}

Перепишем EM"=флгоритм~\ref{eq:03} в соответствии с полученной формулой:
\begin{equation}
    \begin{cases}
        p_{tdw} = \underset{t \in T}{norm}(\phi_{wt}\theta_{td}) \\
        \phi_{wt} = \underset{w \in W}{norm}\left(n_{wt} + \beta_0\beta_w
        \right); \\
        \theta_{td} = \underset{t \in T}{norm}\left(n_{td} + \alpha_0\alpha_t
        \right)
    \end{cases}
\end{equation}

Таким образом был получен модифицированный EM"=алгоритм соответствующий модели
LDA~\cite{teorver, all, pdflec2, videolec2}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Регуляризатор разреживания}
Разреживание подразумевает разделение тем и документов, исключая общие слова из
них. Этот тип регуляризации основывается на предположении, что темы и документы
в основном являются специфичными и описываются относительно небольшим набором
терминов, которые не встречаются в других темах.

Определим регуялризатор разреживания:

Пусть распределения $\phi_{wt}$ далеки от заданного распределения $\beta_w$
и пусть распределения $\theta_{td}$ далеки от заданного распределения
$\alpha_t$.
Тогда в форме KL"=дивергеннции~\ref{sec:02} выразим задачу сглаживания:
\begin{equation}
    \sum_{t \in T} KL(\beta_w||\phi_{wt}) \to \underset{\Phi}{\max}; \;\;
    \sum_{d \in D} KL(\alpha_t||\theta_{td}) \to \underset{\Theta}{\max}.
\end{equation}

Согласно свойству~\ref{it:kl3} KL"=дивергенции перейдём к задаче максимизации
правдоподобия:
\begin{equation}
    R(\Phi, \Theta) = -\beta_o \sum_{t \in T}\sum_{w \in W} \beta_w
	\ln{\phi_{wt}} - \alpha_0 \sum_{d \in D}\sum_{t \in T}\alpha_t
	\ln{\theta_{td}} \to \max.
\end{equation}

Перепишем EM"=флгоритм~\ref{eq:03} в соответствии с полученной формулой:
\begin{equation}
    \begin{cases}
        p_{tdw} = \underset{t \in T}{norm}(\phi_{wt}\theta_{td}) \\
        \phi_{wt} = \underset{w \in W}{norm}\left(n_{wt} - \beta_0\beta_w
        \right); \\
        \theta_{td} = \underset{t \in T}{norm}\left(n_{td} - \alpha_0\alpha_t
        \right)
    \end{cases}
\end{equation}

Таким образом был получен модифицированный EM"=алгоритм, разреживающий
матрицы $\Phi$ и $\Theta$~\cite{teorver, all, pdflec2, videolec2}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Регуляризатор декоррелирования тем}
Декоррелятор тем "--- это частный случай разреживания, призванный выделить
для каждой темы лексическое ядро "--- набор термов, отличающий её от других
тем:

Определим регуляризатор декоррелирования:

Минимизируем ковариации между вектор"=столбцами $\phi_t$:
\begin{equation}
    R(\Phi) = - \frac{\tau}{2} \sum_{t \in T}\sum_{s \in T \backslash t}
    \sum_{w \in W} \phi_{wt}\phi_{ws} \to max.
\end{equation}

Перепишем EM"=флгоритм~\ref{eq:03} в соответствии с полученной формулой:
\begin{equation}
    \begin{cases}
        p_{tdw} = \underset{t \in T}{norm}(\phi_{wt}\theta_{td}) \\
        \phi_{wt} = \underset{w \in W}{norm}\left(n_{wt} - \tau\phi_{wt}
        \sum_{t \in T \backslash t} \phi_{ws}\right); \\
        \theta_{td} = \underset{t \in T}{norm}\left(n_{td} + \theta_{td}
        \frac{\partial R}{\partial \theta_{td}}\right)
    \end{cases}
\end{equation}

Таким образом был получен модифицированный EM"=алгоритм, декоррелирующий
темы~\cite{all, pdflec2, videolec2}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Оценка качества моделей тематического моделирования}
После обучения модели, очевидно, нужно оценить её качество.

Перечислим основные критерии оценки качества тематических моделей:
\begin{enumerate}
    \item Внешние критерии (оценка производится экспертами):
    \begin{enumerate}
        \item Полнота и точность тематического поиска;
        \item Качество ранжирования при тематическом поиске;
        \item Качество классификации / категоризации документов;
        \item Качество суммаризации / сегментации документов;
        \item Экспертные оценки качества тем.
    \end{enumerate}
    \item Внутренние критерии (оценка производится программно):
    \begin{enumerate}
        \item Правдоподобие и перплексия;
        \item Средняя когерентность (согласованность тем);
        \item Разреженность матриц $\Phi$ и $\Theta$;
        \item Различность тем;
        \item Статический тест условной независимости.
    \end{enumerate}
\end{enumerate}

Поскольку оценка по внешним критериям невозможна в рамках данной работы,
сосредоточимся на внутренних критериях оценки, которые можно вычислять
автоматически~\cite{all, pdflec3, videolec3}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Правдоподобия и перплексия}
Перплексия основывается на логарифме правдоподобия и является его некоторой
модификацией.

\begin{equation}
    P(D) = \exp\left(- \frac{1}{n} \sum_{d \in D} \sum_{w \in d} n_{dw}
    \ln{p(w|d)}\right), \;\;\; n = \sum_{d \in D} \sum_{w \in d} n_{dw}
\end{equation}

Не трудно заметить, что при равномерном распределении слов в тексте выполняется
равенство $p(w|d) = \frac{1}{|W|}$. В этом случае значение перплексии равно
мощности словаря $P = |W|$. Это позволяет сделать вывод, что перплексия является
мерой разнообразия и неопределенности слов в тексте: чем меньше значение
перплексии, тем более разнообразны вероятности появления слов.

Таким образом, чем меньше перплексия, тем больше слов с большей вероятностью
$p(w|d)$, которые модель умеет лучше предсказывать, следовательно, чем меньше
перплексия, тем лучше~\cite{teorver, all, pdflec3, videolec3}. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Когерентность}
Когерентность является мерой, коррелирующей с экспертной оценкой
интерпретируемости тем.

Когерентность (согласованность) темы $t$ по $k$ топовым словам:
\begin{equation}
    PNI_t = \frac{2}{k (k - 1)} \sum^{k - 1}_{i = 1} \sum^k_{j = i + 1}
    PMI(w_i, w_j),
\end{equation}
где $w_i$ "--- $i$"=ое слово в порядке убывания $\phi_{wt}$, $PMI(u, v) =
\ln{\frac{|D|N_{uv}}{N_uN_v}}$ "--- поточечная взаимная информация,
$N_{uv}$ "--- число документов, в которых слова $u, v$ хотя бы один раз
встречаются рядом (расстояние опледеляется отдельно), $N_u$ "--- число
документов, в которых $u$ встретился хотя бы один раз.

Гипотезу когерентности можно выразить так: когда человек говорит о какой"=либо
теме, то часто употребляет достаточно ограниченный набор слов, относящийся
к этой теме, следовательно, чем чаще будут встречаться вместе слова этой темы,
тем лучше её можно будет интерпретировать.

Сама когерентность берёт самые часто встречающиеся слова из тем, и вычисляет
для каждой пары из них насколько они часто встречаются, соответственно, чем
выше будет значение взаимовстречаемости,
тем лучше~\cite{all, pdflec3, videolec3}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Разреженность}
Разреженность "--- доля нулевых элементов в матрицах $\Phi$ и $\Theta$.

Разреженность играет ключевую роль в выявлении различий между темами.
Каждая тема формируется на основе ограниченного набора слов, в то время как
остальные слова должны встречаться реже, что отражается в нулевых элементах
матриц. Оптимальный уровень разреженности должен быть высоким, но не чрезмерным:
в таком случае темы будут четко различимы. Если разреженность слишком низка,
темы могут сливаться, а если слишком высока "--- содержать недостаточное
количество слов для адекватного представления.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Чистота темы}
Чистота темы:
\begin{equation}
	\sum_{w \in W_t} p(w|t),
\end{equation}

где $W_t$ "--- ядро темы:
$W_t = \{w: p(w|t) > \alpha\}, \text{где $\alpha$ подбирается по разному,
например $\alpha = 0.25$ или $\alpha = \frac{1}{|W|}$}$.

Данная характеристика показывает как вероятностно относится ядро темы к фоновым
словам темы, следовательно, чем больше вероятность ядра, тем лучше.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Контрастность темы}
Контрастность темы:
\begin{equation}
	\frac{1}{|W_T} \sum_{w \in W_t} p(t|w).
\end{equation}

Данная характеристика показывает насколько часто слова из ядра темы
встречаются в других темах, очевидно, что чем меньше ядро будет встречаться
в других темах, тем лучше. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Тематическое моделирование новостей}
В данном разделе будет выполнено тематическое моделирование новостей новостного
сайта ВШЭ.

Датасет был получен методами парсинга с помощью языка python и библиотек
beuatifulsoap4 и selenium.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Предобработка текстов}
Перед любым моделированием данные нужно подготовить. Вот стандартный набор
предобработки текстов для тематического моделирования:
\begin{itemize}
    \item токенизация;
    \item перевод текста в нижний регистр;
    \item удаление неалфавитных символов;
    \item удаление стоп слов;
    \item лемматизация.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Токенизация, перевод в нижний регистр и удаление неалфавитных символов}
Токенизация "--- это разделение текста на составные части "--- токены
(предложения и слова).

Провести токенизацию можно с помощью средств языка python, библиотека nltk.
За токенизацию отвечают команды:
\begin{minted}{python}
    # разделить текст на предложения
    nltk.sent_tokenize(<sentences>)
    # разделить предложение на слова
    nltk.word_tokenize(<sentence>)
\end{minted}

После того как текст поделен на слова, нужно перевести все слова в нижний
регистр, так как семантическое значение слов, чаще всего, не зависит от
регистра. Перевод в нижний регистр можно с помощью стандартных средств языка
python:
\begin{minted}{python}
    # перевести текст в нижний регистр
    <text>.lower()
\end{minted}

После перевода в нижний регистр нужно удалить все семантически незначимые
символы, в данном случае будем рассматривать в качестве таких символов
все символы, не совпадающие с символами русского и английского алфавитов.
Чтобы провести удаление неалфавитных символов достаточно средств языка python:
\begin{minted}{python}
    new_word = ''
    # перебираем символы некоторого слова
    for symbol in word:
        # если символ принадлежит русскому или английскому алфавитам
        if (symbol >= 'a' and symbol <= 'z' or symbol >= 'а' and symbol <= 'я'):
            # добавляем символ в новое слово
            new_word += symbol
\end{minted}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Удаление стоп"=слов}
Стоп"=слова "--- это слова, которые не несут смысловой нагрузки в рамках,
некоторой темы.

Любой текст содержит большое количество слов общей тематики "--- стоп"=слов.
Такие слова, для улучшения качества модели, можно удалить, так как такие
слова не несут семантической нагрузки, то будут только сбивать модель.

Чтобы удалить стоп"=слова можно воспользоваться библиотки nltk
языка python:
\begin{minted}{python}
    new_words = []
    # перебираем список слов
    for word in words:
        # проверяем какому алфавиту принадлежат символы слова
        if re.match('[а-я]', word):
            # если слово не принадлежит списку стоп слов
            if word not in (stopwords.words('russian')):
                # добавляем слово в новый список слов
                new_words.append(word)
        elif re.match('[a-z]', word):
            # если слово не принадлежит списку стоп слов
            if word not in stopwords.words('english'):
                # добавляем слово в новый список слов
                new_words.append(word)
\end{minted}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Лемматизация}
Лемматизация "--- процесс приведения слова к его начальной форме.

Так как семантическое значение слова для темы не зависит от его формы и падежа,
то перед обучением модели важно привести все слова в начальную форму, сделать
это можно с помощью библиотек nltk и pymorphy2 языка python:
\begin{minted}{python}
    # создаём лемматизаторы
    lemm_nltk = WordNetLemmatizer()
    lemm_pymorphy2 = pymorphy2.MorphAnalyzer()
    
    new_words = []
    # перебираем список слов
    for word in words:
        # проверяем какому алфавиту принадлежат символы слова
        if re.match('[а-я]', word):
            # лемматизируем слово на русском и добавляем его в новый список слов
            new_words.append(lemm_pymorphy2.parse(word)[0].normal_form)
        elif re.match('[a-z]', word):
            # лемматизируем слово на английском и добавляем его в новый список слов
            new_words.append(lemm_nltk.lemmatize(word))
\end{minted}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Создание N"=грамм}
N"=грамма "--- это склеивание слов в словосочетание, слов может быть несколько.

Часто слова в теме встречаются в парах или тройках подряд, тогда, если
склеить слова в N"=грамм, то качество и интерпретируемость моделли может
вырасти.

Сделать N"=граммы можно средствами библиотеки nltk языка python:
\begin{minted}{python}
    n_gramms = []
    # перебираем предложения и составляем список n-грамм
    for sentence in sentences:
        # делаем n граммы и добавляем их в список n-грамм
        n_gramms.append(sentence.split(' '), <n>)
\end{minted}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Статистика по данным}
Чтобы корректнее строить тематические модели нужно знать количественные
характеристики данных, получить такие данные можно удобно с помощью библиотек
pandas и numpy языка python. 

Перечислим некоторые количественные характеристики, характеризующие наш
датасет (перед вычислениями проводилась предобработка данных,
исключая лемматизацию):
\begin{itemize}
    \item количество новостей в датасете: 15768;
    \item средняя длина документа (в словах): 34.6;
    \item медианная длина документа (в словах): 29;
    \item двадцать наиболее популярных слов датасета:
    \begin{enumerate}
        \item вшэ: 11437;
        \item ниу: 5559;
        \item экономики: 4783;
        \item россии: 2955;
        \item высшей: 2498;
        \item школы: 2293;
        \item гувшэ: 2107;
        \item вышки: 2100;
        \item года: 2070;
        \item развития: 2065;
        \item исследований: 1876;
        \item образования: 1858;
        \item году: 1737;
        \item программы: 1644;
        \item студентов: 1481;
        \item факультета: 1428;
        \item университета: 1399;
        \item института: 1307;
        \item школа: 1303;
        \item рамках: 1286.
    \end{enumerate}
\end{itemize}

По этим данным можно сделать следующие выводы:
\begin{itemize}
    \item общий объём данных весьма не велик, что может усложнить
    построение тематической модели;
    \item короткая медианная длина документов тоже приведёт к снижению качества
    модели, так как тематическое моделирование происходит на текстах большей
    длины;
    \item среди 20 наипопулярнейших слов датасета явно присутствуют слова
    общей лексики (стоп"=слова), которые необходимо будет удалить на
    этапе удаления стоп"=слов.
\end{itemize}

Программу вычисляющую количественные характеристики датасета можно найти
в приложениях.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Создание тематической модели с помощью библиотеки BigARTM}
Блок тематических моделей уже реализован в библиотеке BigARTM, которую
можно использовать на языке python.

Модели BigARTM для своей работы требуют особого типа данных "--- vowpal\_wabbit.
Данный тип данных представляет из себя следующую конструкцию.

Преобразовать excel таблицу с новостями к данному формату можно с помощью
стандартных средств языка python и библиотеки pandas:
\begin{minted}{python}
    # считываем excel таблицу в pandas DataFrame
    data = pd.read_excel('news.xlsx')
    # открываем файл для записи vowpal_wabbit файла
    f = open(<path>, 'w')
    # проходимся по строкам DataFrame
    for string in range(data.shape[0]):
        # записываем отдельную новость в файл как отдельный документ
        f.write( 'doc_{0}'.format(string)
               + data.loc[string, 'title']
               + ' '
               + data.loc[string, 'content']
               + '\n')
    # после записи закрываем файл
    f.close()
\end{minted}

Чтобы передать данные из vowpal\_wabbit файла на обучение необходимо создать
батчи, они удобно будут постепенно загружаться в оперативную память по мере
необходимости и передаваться на обучение, кроме того батчи автоматически
вычисляют для себя словарь, который также необходим при обучении. Создать
батчи можно следующим образом:
\begin{minted}{python}
    # data_path - путь к vowpal_wabbit файлу
    # data_format - формат загружаемого файла - vowpal_wabbit
    # batch_size - количество документов в одном батче
    # target_folder - папка, в которую батчи сохраняются
    bv = artm.BatchVectorizer( data_path = 'vw.txt',
                               data_format = 'vowpal_wabbit',
                               batch_size=3000,
                               target_folder='batches' )
\end{minted}

Наконец, можно создать саму модель, делается это следующим образом:
\begin{minted}{python}
    # num_topics - количество тем
    # num_document_passes - количество проходов по каждому документу (новости)
    # dictionary - словарь
    # class_ids - веса для модальностей
    # создание модели
    model = artm.ARTM( num_topics=7,
                       num_document_passes=3,
                       dictionary=bv.dictionary,
                       class_ids={'@default_class': 1.0})
    # добавление метрик
    model.scores.add(artm.PerplexityScore(name='perplexity', dictionary=bv.dictionary))
    # сохранения топа слов для каждой темы
    model.scores.add(artm.TopTokensScore(name='top-tokens', num_tokens=10))
    # добавление регуляризаторов, например, декоррелятора
    # tau - коэффициент регуляризации
    model.regularizers.add(artm.DecorrelatorPhiRegularizer(name='decorrelator', tau=2e7))
\end{minted}

Метрик качества, а также регуляризаторов можно добавить сразу несколько.

После создания модели её нужно обучить, сделать это можно следующим
образо:
\begin{minted}{python}
    for _ in range(<num_passes>):
        model.fit_offline(bv, num_collection_passes=1)
\end{minted}

Чтобы оценить модель можно запросить значение метрик и список слов для тем:
\begin{minted}{python}
    # запрашиваем последнее значение перплексии
    perplexity = model.score_tracker['perplexity'].last_value
    # запрашиваем массив самых популярных слов для каждой темы
    top_tokens = model.score_tracker['top-tokens'].last_value
\end{minted}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{PLSA (модель без регуляризаторов)}
Модели PLSA соответствует EM"=алгоритм без регуляризаторов. Данную модель
можно создать средствами библиотеки BigARTM следующим образом:
\begin{minted}{python}
    model = artm.ARTM( num_topics=param1,
                       num_document_passes=param3,
                       dictionary=bv.dictionary,
                       class_ids={'@default_class': 1.0} )

    model.scores.add(artm.PerplexityScore(name='perplexity', dictionary=bv.dictionary))
    model.scores.add(artm.SparsityPhiScore(name='sparsity_phi_score'))
    model.scores.add(artm.SparsityThetaScore(name='sparsity_theta_score'))
    model.scores.add(artm.TopTokensScore(name='top-tokens', num_tokens=10))
\end{minted}

Для оценки качества модели выбраны такие характеристика как перплексия и
разреженность (по матрицам $\Phi$ и $\Theta$).

На место параметров модели (param1, param3) в функции создания и обучения,
которую можно увидеть в приложениях, будут подставляться значения из некоторого
набора, затем модель будет обучаться param2 раз. После обучения будет выведена
соответствующая таблица с результатами по моделям.

В результате получаем следующую таблицу:

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{LDA (модель с регуляризатором сглаживания)}
Модели LDA соответствует EM"=алгоритм с регуляризатором сглаживания.
Создать модель можно следующим образом:
\begin{minted}{python}
    model = artm.ARTM( num_topics=param1,
                       num_document_passes=param3,
                       dictionary=bv.dictionary,
                       class_ids={'@default_class': 1.0} )

    model.scores.add(artm.PerplexityScore(name='perplexity', dictionary=bv.dictionary))
    model.scores.add(artm.SparsityPhiScore(name='sparsity_phi_score'))
    model.scores.add(artm.SparsityThetaScore(name='sparsity_theta_score'))
    model.scores.add(artm.TopTokensScore(name='top-tokens', num_tokens=10))
\end{minted}

Для оценки качества модели выбраны такие же характеристки как и у модели PLSA.

На место параметров модели (param1, param3) в функции создания и обучения,
которую можно увидеть в приложениях, будут подставляться значения из некоторого
набора, затем модель будет обучаться param2 раз. После обучения будет выведена
соответствующая таблица с результатами по моделям.

В результате получаем следующую таблицу:

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Модель с регуляризатором разреживания}
В данном случае модели соответствует EM"=алгоритм с регуляризатором
разреживания. Создать модель можно следующим образом:
\begin{minted}{python}
    model = artm.ARTM( num_topics=param1,
                       num_document_passes=param3,
                       dictionary=bv.dictionary,
                       class_ids={'@default_class': 1.0} )

    model.scores.add(artm.PerplexityScore(name='perplexity', dictionary=bv.dictionary))
    model.scores.add(artm.SparsityPhiScore(name='sparsity_phi_score'))
    model.scores.add(artm.SparsityThetaScore(name='sparsity_theta_score'))
    model.scores.add(artm.TopTokensScore(name='top-tokens', num_tokens=10))
\end{minted}

Характеристики для оценки качества используются всё теже.

На место параметров модели (param1, param3) в функции создания и обучения,
которую можно увидеть в приложениях, будут подставляться значения из некоторого
набора, затем модель будет обучаться param2 раз. После обучения будет выведена
соответствующая таблица с результатами по моделям.

В результате получаем следующую таблицу:

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Модель с регуляризатором декоррелирования}
В данном случае модели соответствует EM"=алгоритм с регуляризатором
декоррелирования. Создать модель можно следующим образом:
\begin{minted}{python}
    model = artm.ARTM( num_topics=param1,
                       num_document_passes=param3,
                       dictionary=bv.dictionary,
                       class_ids={'@default_class': 1.0} )

    model.scores.add(artm.PerplexityScore(name='perplexity', dictionary=bv.dictionary))
    model.scores.add(artm.SparsityPhiScore(name='sparsity_phi_score'))
    model.scores.add(artm.SparsityThetaScore(name='sparsity_theta_score'))
    model.scores.add(artm.TopTokensScore(name='top-tokens', num_tokens=10))
\end{minted}

Характеристики для оценки качества используются всё теже.

На место параметров модели (param1, param3) в функции создания и обучения,
которую можно увидеть в приложениях, будут подставляться значения из некоторого
набора, затем модель будет обучаться param2 раз. После обучения будет выведена
соответствующая таблица с результатами по моделям.

В результате получаем следующую таблицу:

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Выбор лучшей модели}
В работе

% Раздел "Заключение"
\conclusion
В работе


%Библиографический список, составленный вручную, без использования BibTeX
%
%\begin{thebibliography}{99}
%  \bibitem{Ione} Источник 1.
%  \bibitem{Itwo} Источник 2
%\end{thebibliography}

%Библиографический список, составленный с помощью BibTeX
%
\bibliographystyle{gost780uv}
\bibliography{thesis}

% Окончание основного документа и начало приложений
% Каждая последующая секция документа будет являться приложением
%\appendix

%\section{Нумеруемые объекты в приложении}


\end{document}
