\documentclass[bachelor, och, coursework]{SCWorks}
% параметр - тип обучения - одно из значений:
%    spec     - специальность
%    bachelor - бакалавриат (по умолчанию)
%    master   - магистратура
% параметр - форма обучения - одно из значений:
%    och   - очное (по умолчанию)
%    zaoch - заочное
% параметр - тип работы - одно из значений:
%    referat    - реферат
%    coursework - курсовая работа (по умолчанию)
%    diploma    - дипломная работа
%    pract      - отчет по практике
%    pract      - отчет о научно-исследовательской работе
%    autoref    - автореферат выпускной работы
%    assignment - задание на выпускную квалификационную работу
%    review     - отзыв руководителя
%    critique   - рецензия на выпускную работу
% параметр - включение шрифта
%    times    - включение шрифта Times New Roman (если установлен)
%               по умолчанию выключен
\usepackage[T2A]{fontenc}
\usepackage[cp1251]{inputenc}
\usepackage{graphicx}

\usepackage[sort,compress]{cite}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{fancyvrb}
\usepackage{longtable}
\usepackage{array}
\usepackage[english,russian]{babel}
\usepackage{tempora}
\usepackage{minted}
\usepackage{cancel}

\usepackage[colorlinks=true]{hyperref}


\newcommand{\eqdef}{\stackrel {\rm def}{=}}

\newtheorem{lem}{Лемма}

\begin{document}

% Кафедра (в родительном падеже)
\chair{математической кибернетики и компьютерных наук}

% Тема работы
\title{Создание фреймворка для работы с автоматизированным тестированием}

% Курс
\course{3}

% Группа
\group{351}

% Факультет (в родительном падеже) (по умолчанию "факультета КНиИТ")
\department{факультета КНиИТ}

% Специальность/направление код - наименование
\napravlenie{09.03.04 "--- Программная инженерия}

% Для студентки. Для работы студента следующая команда не нужна.
\studenttitle{Студента}

% Фамилия, имя, отчество в родительном падеже
\author{Кондрашова Даниила Владиславовича}

% Заведующий кафедрой
\chtitle{к.\,ф.-м.\,н.} % степень, звание
\chname{С.\,В.\,Миронов}

%Научный руководитель (для реферата преподаватель проверяющий работу)
\satitle{доцент, к.\,ф.-м.\,н.} %должность, степень, звание
\saname{С.\,В.\,Папшев}

% Год выполнения отчета
\date{2024}

\maketitle

% Включение нумерации рисунков, формул и таблиц по разделам
% (по умолчанию - нумерация сквозная)
% (допускается оба вида нумерации)
%\secNumbering


\tableofcontents

% Раздел "Обозначения и сокращения". Может отсутствовать в работе


% Раздел "Определения". Может отсутствовать в работе
%\definitions

% Раздел "Определения, обозначения и сокращения". Может отсутствовать в работе.
% Если присутствует, то заменяет собой разделы "Обозначения и сокращения" и "Определения"
%\defabbr


% Раздел "Введение"
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\intro
С ростом объёмов информации в наше время для её эффективного поиска и изучения
стало необходимо уметь её классифицировать и структурировать. Сегодня физически
невозможно найти нужные данные просто перебирая все ресурсы подряд, появилась
острая потребность в поиске по темам, в классификации данных.

Данную проблему призвано решить тематическое моделирование. Оно способно
бытро и эффективно автоматически разбить по темам огромные объёмы информации.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Математические основы тематического моделирования}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Основная гипотеза тематического моделирования}
Тематическое моделирование "--- это подход анализа текстовых данных,
направленный на выявление семантических структур в коллекции документов.

Само тематическое моделирование основывается на предположении, что слова в
тексте связаны
не с документом, а с темой. Кроме того первично текст разбивается на темы, затем
каждая из них порождает слово для соответствующих позиций в документе.
Таким образом, сначала порождается тема, а потом термины.

Благодаря этой гипотезе можно по частоте и взаимовстречаемости слов производить
тематическую классификацию текстов.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Аксиоматика тематического моделирования}
Каждый текст можно количественно охарактеризовать. Вот основные количественные
характеристики, использующиеся при тематическом моделировании:
\begin{itemize}
    \item $W$ "--- конечное множество термов;
    \item $D$ "--- конечное множество текстовых документов;
    \item $T$ "--- конечное множество тем;
    \item $D \times W \times T$ "--- дискретное вероятностное пространство;
    \item коллекция "--- i.i.d выборка $(d_i, w_i, t_i)^n_{i = 1}$;
    \item $n_{dwt} = \sum^n_{i = 1} [d_i = d][w_i = w][t_i = t]$ "--- частота
    $(d, w, t)$ в коллекции;
    \item $n_{wt} = \sum_d n_{dwt}$ "--- частота терма $w$ в документе $d$;
    \item $n_{td} = \sum_w n_{dwt}$ "--- частота термов темы $t$ в документе
    $d$;
    \item $n_t = \sum_{d, w} n_{dwt}$ "--- частота термов темы $t$ в коллекции;
    \item $n_{dw} = \sum_t n_{dwt}$ "--- частота терма $w$ в документе $d$;
    \item $n_W = \sum_d n_{dw}$ "--- частота терма $w$ в коллекции;
    \item $n_d = \sum_w n_{dw}$ "--- длина документа $d$;
    \item $n = \sum_{d, w} n_{dw}$ "--- длина коллекции.
\end{itemize}

Также в тематическом моделировании используются следующие гипотезы и аксиомы:
\begin{itemize}
    \item Независимость слов от порядка в документе: порядок слов в документе
    не важен;
    \item Независимость от порядка документов в коллекции: порядок документов
    в коллекции не важен;
    \item Зависимость терма от темы: каждый терм связан с соответствующей темой
    и порождается ей;
    \item Гипотеза условной независимости: $p(w|d, t) = p(w|t)$.
\end{itemize}

Вышеперечисленные характеристи, гипотезы и аксиомы являются основой
тематического моделирования, являющейся достаточной для построения тематической
модели.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Задача тематического моделирования}
Как уже говорилось ранее, документ порождается следующим образом:
\begin{enumerate}
    \item для каждой позиции в документе генерируется тема $p(t|d)$;
    \item для каждой сгенерированной темы в соответствующей позиции генерируем
    терм $p(w|d, t)$.
\end{enumerate}

Тогда вероятность появления слова в документе можно описать по формуле полной
вероятности:
\begin{equation}
    p(w|d) = \sum_{t \in T} p(w|d,t)p(t|d) = \sum_{t \in T} p(w|t)p(t|d) 
\end{equation}

Такой алгоритм является прямой задачей порождения текста. Тематическое
моделирование призвано решить обратную задачу:
\begin{enumerate}
    \item для каждого терма $w$ в тексте найти вероятность появления в теме $t$
    (найти $p(w|t) = \phi_{wt}$);
    \item для каждой темы $t$ найти вероятность появления в документе $d$
    (найти $p(t|d) = \theta_{td}$).
\end{enumerate}

Обратную задачу можно представить в виде стохастического матричного разложения.

Таким образом, тематическое моделирование ищет величину $p(w|d)$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Решение обратной задачи}
Для решения задачи тематического моделирования необходимо найти величину
$p(w|d)$, сделать это можно с помощью метода максимального правдоподобия.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Лемма о максимизации функции на единичных симплексах}
Перед выведением решения обратной задачи выпишем лемму, позволяющую это решение
найти.

Введём операцию нормировки вектора:
\begin{equation}
    p_i = \underset{i \in I}(x_i) =
    \frac{\max{x_i, 0}}{\sum_{k \in I} \max{x_k, 0}}
\end{equation}

\textbf{Лемма о максимизации функции на единичных симплексах:}

Пусть функция $f(\Omega)$ непрерывно дифференцируема по набору векторов
$\Omega = (w_i)_{j \in J}, \;\; w_j = (w_{ij})_{i \in I_j}$ различных
размерностей $|I_j|$. Тогда векторы $w_j$ локального экстремума задачи
\begin{equation*}
    \begin{cases}
        f(\Omega) \to \underset{\Omega}{\max} \\
        \sum_{i \in I_j} w_{ij} = 1, \;\; j \in J \\
        w_{ij} \geq 0, \;\; i \in I_j, j \in J
    \end{cases}
\end{equation*}

при условии $1^0: \;\; (\exists i \in I_j) w_{ij}
\frac{\partial f}{\partial w_{ij}} > 0$ удовлетворяют уравнениям
\begin{equation}
    w_{ij} = \underset{i \in I_j}{norm}\left(w_{ij}
    \frac{\partial f}{\partial w_{ij}}\right), \;\; i \in I_j;
\end{equation}

при условии $2^0: \;\; (\forall i \in I_j) w_{ij}
\frac{\partial f}{\partial w_{ij}} \leq 0$ и $(\exists i \in I_j) w_{ij}
\frac{\partial f}{\partial w_{ij}} < 0$ удовлетворяют уравнениям
\begin{equation}
    w_{ij} = \underset{i \in I_j}{norm}\left(-w_{ij}
    \frac{\partial f}{\partial w_{ij}}\right), \;\; i \in I_j;
\end{equation}

в противном случае (условие $3^0$) "--- однородным уравнениям
\begin{equation}
    w_{ij}\frac{\partial f}{\partial w_{ij}} = 0, \;\; i \in I_j.
\end{equation}

Данная лемма служит для оптимизации любых моделей, параметрами которых являются
неотрицательные нормированные векторы.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Сведение обратной задачи к задаче максимизации функционала}
Чтобы вычислить величину $p(w|d)$ воспользуемся принципом максимума
правдоподобия, согласно которому будут подобраны параметры $\Phi, \;\;\Theta$
такие, что $p(w|d)$ примет наибольшее значение.

\begin{equation}
    \prod^n_{i = 1} p(d_i, w_i) = \prod_{d \in D} \prod_{w \in d} p(d, w)^{
        n_{dw}}
\end{equation}

Прологарифмировав правдоподобие, перейдём к задаче максимизации логарифма 
правдоподобия.
\begin{equation}
    \sum_{d \in D} \sum_{w \in d} n_{dw} \ln{p(w|d)
    \underset{const}{\xcancel{p(d)}}} = n_{dw} \to max
\end{equation}

Данная задача эквивалентна задаче максимизации функционала
\begin{equation}
    L(\Phi, \Theta) = \sum_{d \in D} \sum_{w \in d} n_{dw} \ln{\sum_{t \in T}
    \phi_{wt} \theta_{td}} \to \underset{\Phi, \Theta}{max}
\end{equation}

при ограничениях неотрицательности и нормировки
\begin{equation}
    \phi_{wt} \geq 0; \;\; \sum_{w \in W} \phi_{wt} = 1; \;\;\; \theta_{td}
\geq 0; \;\; \sum_{t \in T} \theta_{td} = 1
\end{equation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Аддитивная регуляризация тематических моделей}
Задача не соответствует определению корректно поставленной задачи по Адамару,
так как она в общем случае имеет бесконечно много решений, следовательно задачу
нужно доопределить.

Для доопределения некорректно поставленных задач используют регуляризацию:
к основному критерию добавляют дополнительный критерий "--- регуляризатор,
соответствующий решаемой задаче.

ARTM: аддитивная регуляризация тематических моделей основана на максимизации
линейной комбинации логарифма правдоподобия и регуляризаторов
$R_i(\Phi, \Theta)$ с неотрицательными коэффициентами регуляризации $t\tau_i, \;
\; i = 1, \dots, k$.

Преобразуем задачу к ARTM виду:
\begin{equation}
    \sum_{d \in D} \sum_{w \in d} n_{dw} \ln{\sum_{t \in T}
    \phi_{wt} \theta_{td}} + R(\Phi, \Theta) \to \underset{\Phi, \Theta}{max};
    \;\; R(\Phi, \Theta) = \sum^k_{i = 1} \tau_i R_i(\Phi, \Theta)
\end{equation}

при ограничениях неотрицательности и нормировки.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{E"=M алгоритм}
Из ограничений видно, что столбцы матриц можно принять за неотрицательные
единичные векторы, а, следовательно, задача является задачей максимизации
функции на единичных симплексах.

Воспользуемся леммой о максимизации функции на единичных симплексах и перепишем
задачу.

Пусть функция $R(\Phi, \Theta)$ непрерывно дифференцируема. Тогда точка $(\Phi,
\Theta)$ локального экстремума задачи с ограничениями, удовлетворяет системе
уравнений сос вспомогательными переменными $p_{twd} = p(t|d, w)$, если из
решения исключить нулевые столбцы матриц $\Phi$ и $\Theta$:

\begin{equation}
    \begin{cases}
        p_{tdw} = \underset{t \in T}{norm}(\phi_{wt}\theta_{td}) \\
        \phi_{wt} = \underset{w \in W}{norm}\left(n_{wt} + \phi_{wt}
        \frac{\partial R}{\partial \phi_{wt}}\right); \\
        \theta_{td} = \underset{t \in T}{norm}\left(n_{td} + \theta_{td}
        \frac{\partial R}{\partial \theta_{td}}\right)
    \end{cases}
\end{equation}

Полученная модель соответствует E"=M алгоритму, где первая строка системы
уравнений соответствует E шагу, а вторая и третья строки "--- M шагу.

Решив полученную систему уравнений, методом простых итерации получим искомые
матрицы $\Phi$ и $\Theta$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Регуляризаторы в тематическом моделировании}
В этом разделе будут рассмотрены некоторые возможные вариантор регуляризаторов.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Дивергенция Кульбака"=Лейблера}
Чтобы оцень близость тем можно воспользователься дивергенцией
Кульбака"=Лейблера (KL или KL"=дивергенция). KL"=дивергенция позволяет оценить
степень вложенности одного распределения в другое, в случае тематического
моделирования будет оценитьваться вложенность матриц.

Определим KL"=дивергенцию:

Пусть $P = (p_i)^n_{i = 1}$ и $Q = (q_i)^n_{i = 1}$ некоторые распределения.
Тогда дивергенция Кульбака"=Лейблера имеет следующий вид:
\begin{equation}
    KL(P||Q) = KL_i(p_i||q_i) = \sum^n_{i = 1} p_i \ln{\frac{p_i}{q_i}}.
\end{equation}

Свойства KL"=дивергенции:
\begin{enumerate}
    \item $KL(P||Q) \geq 0$;
    \item $KL(P||Q) = 0 \;\; \Leftrightarrow \;\; P = Q$;
    \item Минимизация KL эквивалентна максимизации правдоподобия:
    $$KL(P||Q(\alpha)) = \sum^n_{i = 1} p_i \ln{\frac{p_i}{q_i(\alpha)}} \to
    \underset{\alpha}{\min} \;\; \Leftrightarrow \;\; \sum^n_{i = 1} p_i
    \ln{q_i}(\alpha) \to \underset{\alpha}{\max};$$
    \item Если $KL(P||Q) < KL(Q||P)$, то $P$ сильнее вложено в $Q$, чем $Q$ в
    $P$.
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Регуляризатор сглаживания}
Сглаживание предполагает сближение тем, это может быть полезно в следующих
случаях:
\begin{enumerate}
    \item Темы могут быть похожи между собой по терминологии, например,
    основы теории вероятностей и линейной алгебры обладают рядом одинаковых
    терминов;
    \item При выделении фоновых тем важно максимально вобрать в них слова,
    следовательно, сглаживание поможет решить эту задачу.
\end{enumerate}

Определим регуляризатор сглаживания:

Пусть распределения $\phi_{wt}$ близки к заданному распределению $\beta_w$
и пусть распределения $\theta_{td}$ близки к заданному распределению $\alpha_t$.
Тогда в форме KL"=дивергеннции выразим задачу сглаживания:
\begin{equation}
    \sum_{t \in T} KL(\beta_w||\phi_{wt}) \to \underset{\Phi}{\min}; \;\;
    \sum_{d \in D} KL(\alpha_t||\theta_{td}) \to \underset{\Theta}{\min}.
\end{equation}

Согласно свойству 3 KL"=дивергенции перейдём к задаче максимизации
правдоподобия:
\begin{equation}
    R(\Phi, \Theta) = \beta_o \sum_{t \in T}\sum_{w \in W} \beta_w \ln{\phi_wt}
    + \alpha_0 \sum_{d \in D}\sum_{t \in T}\alpha_t \ln{\theta_{td}} \to \max.
\end{equation}

Перепишем EM"=флгоритм в соответствии с полученной формулой:
\begin{equation}
    \begin{cases}
        p_{tdw} = \underset{t \in T}{norm}(\phi_{wt}\theta_{td}) \\
        \phi_{wt} = \underset{w \in W}{norm}\left(n_{wt} + \beta_0\beta_w
        \right); \\
        \theta_{td} = \underset{t \in T}{norm}\left(n_{td} + \alpha_0\alpha_t
        \right)
    \end{cases}
\end{equation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Регуляризатор разреживания}
Разреживание предполагает разделение тем и документов, исключение из них общих
слов. Данный тип регуляризации отталкивается от того, что в большинстве своём
темы и документы специфичны и описываются относительно небольшим набором
терминов, не встречающихся в других темах.

Определим регуялризатор разреживания:

Пусть распределения $\phi_{wt}$ далеки от заданного распределения $\beta_w$
и пусть распределения $\theta_{td}$ далеки от заданного распределения
$\alpha_t$.
Тогда в форме KL"=дивергеннции выразим задачу сглаживания:
\begin{equation}
    \sum_{t \in T} KL(\beta_w||\phi_{wt}) \to \underset{\Phi}{\max}; \;\;
    \sum_{d \in D} KL(\alpha_t||\theta_{td}) \to \underset{\Theta}{\max}.
\end{equation}

Согласно свойству 3 KL"=дивергенции перейдём к задаче максимизации
правдоподобия:
\begin{equation}
    R(\Phi, \Theta) = -\beta_o \sum_{t \in T}\sum_{w \in W} \beta_w \ln{\phi_wt}
    - \alpha_0 \sum_{d \in D}\sum_{t \in T}\alpha_t \ln{\theta_{td}} \to \max.
\end{equation}

Перепишем EM"=флгоритм в соответствии с полученной формулой:
\begin{equation}
    \begin{cases}
        p_{tdw} = \underset{t \in T}{norm}(\phi_{wt}\theta_{td}) \\
        \phi_{wt} = \underset{w \in W}{norm}\left(n_{wt} - \beta_0\beta_w
        \right); \\
        \theta_{td} = \underset{t \in T}{norm}\left(n_{td} - \alpha_0\alpha_t
        \right)
    \end{cases}
\end{equation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Регуляризатор декоррелирования тем}
Декоррелятор тем "--- это частный случай разреживания, призванный выделить
для каждой темы лексическое ядро "--- набор термов, отличающий её от других
тем:

Определим регуляризатор декоррелирования:

Минимизируем ковариации между вектор"=столбцами $\phi_t$:
\begin{equation}
    R(\Phi) = - \frac{\tau}{2} \sum_{t \in T}\sum_{s \in T \backslash t}
    \sum_{w \in W} \phi_{wt}\phi_{ws} \to max.
\end{equation}

Перепишем EM"=флгоритм в соответствии с полученной формулой:
\begin{equation}
    \begin{cases}
        p_{tdw} = \underset{t \in T}{norm}(\phi_{wt}\theta_{td}) \\
        \phi_{wt} = \underset{w \in W}{norm}\left(n_{wt} - \tau\phi_{wt}
        \sum_{t \in T \backslash t} \phi_{ws}\right); \\
        \theta_{td} = \underset{t \in T}{norm}\left(n_{td} + \theta_{td}
        \frac{\partial R}{\partial \theta_{td}}\right)
    \end{cases}
\end{equation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Оценка качества моделей тематического моделирования}
После обучения модели, очевидно, нужно оценить её качество.

Перечислим основные критерии оценки качества тематических моделей:
\begin{enumerate}
    \item Внешние критерии (оценка производится экспертами):
    \begin{enumerate}
        \item Полнота и точность тематического поиска;
        \item Качество ранжирования при тематическом поиске;
        \item Качество классификации / категоризации документов;
        \item Качество суммаризации / сегментации документов;
        \item Экспертные оценки качества тем.
    \end{enumerate}
    \item Внутренние критерии (оценка производится программно):
    \begin{enumerate}
        \item Правдоподобие и перплексия;
        \item Средняя когерентность (согласованность тем);
        \item Разреженность матриц $\Phi$ и $\Theta$;
        \item Различность тем;
        \item Статический тест условной независимости.
    \end{enumerate}
\end{enumerate}

Так как оценка по внешним критериям не представляется возможной в рамках данной
работы, то рассмотрим внутренние критерии оценки, так как их можно вычислять
автоматически.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Правдоподобия и перплексия}
Перплексия основывается на логарифме правдоподобия и является его некоторой
модификацией.

\begin{equation}
    P(D) = \exp\left(- \frac{1}{n} \sum_{d \in D} \sum_{w \in d} n_{dw}
    \ln{p(w|d)}\right), \;\;\; n = \sum_{d \in D} \sum_{w \in d} n_{dw}
\end{equation}

Не трудно заметить, что, при равномерном распределении слов в тексте
$p(w|d) = \frac{1}{|W|}$, значение преплексии равно мощности словаря $P = |W|$.
Тогда можно сделать, вывод, что перплексия "--- это мера различности и
неопределённости слов в тексте, то есть, чем меньше перплексия, тем различнее
вероятности появления слов в тексте.

Таким образом, чем меньше перплексия, тем больше слов с большей вероятностью
$p(w|d)$, которые модель умеет лучше предсказывать, следовательно, чем меньше
перплексия, тем лучше. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Когерентность}
Когерентность является мерой, коррелирующей с экспертной оценкой
интерпретируемости тем, которую можео вычислять автоматически.

Когерентность (согласованность) темы $t$ по $k$ топовым словам:
\begin{equation}
    PNI_t = \frac{2}{k (k - 1)} \sum^{k - 1}_{i = 1} \sum^k_{j = i + 1}
    PMI(w_i, w_j),
\end{equation}
где $w_i$ "--- $i$"=ое слово в порядке убывания $\phi_{wt}$, $PMI(u, v) =
\ln{\frac{|D|N_{uv}}{N_uN_v}}$ "--- поточечная взаимная информация,
$N_{uv}$ "--- число документов, в которых слова $u, v$ хотя бы один раз
встречаются рядом (расстояние опледеляется отдельно), $N_u$ "--- число
документов, в которых $u$ встретился хотя бы один раз.

Гипотезу когерентности можно выразить так: когда человек говорит о какой"=либо
теме, то часто употребляет достаточно ограниченный набор слов, относящийся
к этой теме, следовательно, чем чаще будут встречаться вместе слова этой темы,
тем лучше её можно будет интерпретировать.

Сама когерентность берёт самые часто встречающиеся слова из тем, и вычисляет
для каждой пары из них насколько они часто встречаются, соответственно, чем
выше будет значение взаимовстречаемости, тем лучше.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Разреженность и различность}
Разреженность "--- доля нулевых элементов в матрицах $\Phi$ и $\Theta$.
Разреженность служит для выявления различности тем, так как каждая тема
состоит из небольшого набора слов, то и остальные слова в ней должны встречаться
нечасто, что соответствует нулевым элементам в матрицах. Разреженность должна
быть в рамках оптимальных значений, высокой, но не слишком, тогда темы будут
хорошо различимы, в противном случае, они либо не будут различаться
(разреженность слишком низкая), либо будут содержать слишком мало слов
(разреженность слишком высокая).

\begin{itemize}
    \item Чистота темы: $\sum_{w \in W_t} p(w|t)$, где $W_t$ "--- ядро темы:
    $W_t = \{w: p(w|t) > \alpha\}, \text{где $\alpha$ подбирается по разному,
    например $\alpha = 0.25$ или $\alpha = \frac{1}{|W|}$}$. Данная
    характеристика показывает как вероятностно относится ядро темы к фоновым
    словам темы, следовательно, чем больше вероятность ядра, тем лучше;
    \item Контрастность темы: $\frac{1}{|W_T} \sum_{w \in W_t} p(t|w)$.
    Данная характеристика показывает насколько часто слова из ядра темы
    встречаются в других темах, очевидно, что чем меньше ядро будет встречаться
    в других темах, тем лучше. 
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Тематическое моделирование новостей}

% Раздел "Заключение"
\conclusion



%Библиографический список, составленный вручную, без использования BibTeX
%
%\begin{thebibliography}{99}
%  \bibitem{Ione} Источник 1.
%  \bibitem{Itwo} Источник 2
%\end{thebibliography}

%Библиографический список, составленный с помощью BibTeX
%
\bibliographystyle{gost780uv}
\bibliography{thesis}

% Окончание основного документа и начало приложений
% Каждая последующая секция документа будет являться приложением
\appendix


\end{document}
