\documentclass[bachelor, och, coursework]{SCWorks}
% параметр - тип обучения - одно из значений:
%    spec     - специальность
%    bachelor - бакалавриат (по умолчанию)
%    master   - магистратура
% параметр - форма обучения - одно из значений:
%    och   - очное (по умолчанию)
%    zaoch - заочное
% параметр - тип работы - одно из значений:
%    referat    - реферат
%    coursework - курсовая работа (по умолчанию)
%    diploma    - дипломная работа
%    pract      - отчет по практике
%    pract      - отчет о научно-исследовательской работе
%    autoref    - автореферат выпускной работы
%    assignment - задание на выпускную квалификационную работу
%    review     - отзыв руководителя
%    critique   - рецензия на выпускную работу
% параметр - включение шрифта
%    times    - включение шрифта Times New Roman (если установлен)
%               по умолчанию выключен
\usepackage[T2A]{fontenc}
\usepackage[cp1251]{inputenc}
\usepackage{graphicx}

\usepackage[sort,compress]{cite}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{fancyvrb}
\usepackage{longtable}
\usepackage{array}
\usepackage[english,russian]{babel}
\usepackage{tempora}
\usepackage{minted}
\usepackage{cancel}

\usepackage[colorlinks=true]{hyperref}


\newcommand{\eqdef}{\stackrel {\rm def}{=}}

\newtheorem{lem}{Лемма}

\begin{document}

% Кафедра (в родительном падеже)
\chair{математической кибернетики и компьютерных наук}

% Тема работы
\title{Тематическое моделирование новостей}

% Курс
\course{3}

% Группа
\group{351}

% Факультет (в родительном падеже) (по умолчанию "факультета КНиИТ")
%\department{факультета КНиИТ}

% Специальность/направление код - наименование
%\napravlenie{02.03.02 "--- Фундаментальная информатика и информационные технологии}
%\napravlenie{02.03.01 "--- Математическое обеспечение и администрирование информационных систем}
%\napravlenie{09.03.01 "--- Информатика и вычислительная техника}
\napravlenie{09.03.04 "--- Программная инженерия}
%\napravlenie{10.05.01 "--- Компьютерная безопасность}

% Для студентки. Для работы студента следующая команда не нужна.
%\studenttitle{Студентки}

% Фамилия, имя, отчество в родительном падеже
\author{Кондрашова Даниила Владиславовича}

% Заведующий кафедрой
\chtitle{к.\,ф.-м.\,н.} % степень, звание
\chname{С.\,В.\,Миронов}

%Научный руководитель (для реферата преподаватель проверяющий работу)
\satitle{доцент, к.\,ф.-м.\,н.} %должность, степень, звание
\saname{С.\,В.\,Папшев}

% Руководитель практики от организации (только для практики,
% для остальных типов работ не используется)
%\patitle{к.\,ф.-м.\,н., доцент}
%\paname{Д.\,Ю.\,Петров}

% Семестр (только для практики, для остальных
% типов работ не используется)
%\term{2}

% Наименование практики (только для практики, для остальных
% типов работ не используется)
%\practtype{учебная}

% Продолжительность практики (количество недель) (только для практики,
% для остальных типов работ не используется)
%\duration{2}

% Даты начала и окончания практики (только для практики, для остальных
% типов работ не используется)
%\practStart{01.07.2016}
%\practFinish{14.07.2016}

% Год выполнения отчета
\date{2024}

\maketitle

% Включение нумерации рисунков, формул и таблиц по разделам
% (по умолчанию - нумерация сквозная)
% (допускается оба вида нумерации)
%\secNumbering


\tableofcontents

% Раздел "Обозначения и сокращения". Может отсутствовать в работе
%\abbreviations

% Раздел "Определения". Может отсутствовать в работе
%\definitions

% Раздел "Определения, обозначения и сокращения". Может отсутствовать в работе.
% Если присутствует, то заменяет собой разделы "Обозначения и сокращения" и "Определения"
%\defabbr


% Раздел "Введение"
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\intro
С ростом объёмов информации в современном мире умение классифицировать и
структурировать данные становится необходимым для их эффективного поиска и
изучения. Физически невозможно найти нужные сведения, просто перебирая все
ресурсы подряд, поэтому возникает острая потребность в их классификации.

Тематическое моделирование способствует решению данной проблемы.
Оно позволяет полуавтоматически разбивать большие объёмы информации по темам,
упрощая тем самым анализ данных.

% Абзац с актуальностью под вопросом
% Актуальность данной темы обусловлена быстрыми темпами роста объёма данных,
% которые нужно уже здесь и сейчас быстро и эффективно классифицировать и
% структурировать, а также вести точный и быстрый поиск по ним.
Актуальность данной темы обусловлена потребностью скорейшего решения задачи
анализа больших объёмов данных.

Целью данной курсовой работы является создание тематической модели для
тематического моделирования новостей. Работа включает в себя изучение
теоретических принципов тематического моделирования, создание тематических
моделей, их анализ и выбор лучшей из них.

В ходе данной работы будут решены следующие задачи:
\begin{itemize}
    \item изучение теоретических основ тематического моделирования;
    \item изучение методов предобработки данных для тематического моделирования;
    \item разработка тематических моделей четырёх видов средствами библиотеки
    BigARTM;
    \item сравнительный анализ качества полученных тематических моделей.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Математические основы тематического моделирования}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Основная гипотеза тематического моделирования}
Тематическое моделирование "--- это метод анализа текстовых данных, который
позволяет выявлять семантические структуры в коллекциях документов.

Основная идея тематического моделирования~\cite{all} заключается в том, что
слова в тексте связаны не с конкретным документом, а с темами. Сначала текст
разбивается на темы, и каждая из них генерирует слова для соответствующих
позиций в документе. Таким образом, сначала формируется тема, а замет тема
формирует терм.

Эта гипотеза позволяет проводить тематическую классификацию текстов на основе
частоты и взаимовстречаемости слов.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Аксиоматика тематического моделирования}
Каждый текст можно количественно охарактеризовать. Ниже приведены
основные количественные характеристики, использующиеся при тематическом
моделировании~\cite{ARTM}:
\begin{itemize}
    \item $W$ "--- конечное множество термов;
    \item $D$ "--- конечное множество текстовых документов;
    \item $T$ "--- конечное множество тем;
    \item $D \times W \times T$ "--- дискретное вероятностное пространство;
    \item коллекция "--- i.i.d выборка $(d_i, w_i, t_i)^n_{i = 1}$;
    \item $n_{dwt} = \sum^n_{i = 1} [d_i = d][w_i = w][t_i = t]$ "--- частота
    $(d, w, t)$ в коллекции;
    \item $n_{wt} = \sum_d n_{dwt}$ "--- частота терма $w$ в документе $d$;
    \item $n_{td} = \sum_w n_{dwt}$ "--- частота термов темы $t$ в документе
    $d$;
    \item $n_t = \sum_{d, w} n_{dwt}$ "--- частота термов темы $t$ в коллекции;
    \item $n_{dw} = \sum_t n_{dwt}$ "--- частота терма $w$ в документе $d$;
    \item $n_W = \sum_d n_{dw}$ "--- частота терма $w$ в коллекции;
    \item $n_d = \sum_w n_{dw}$ "--- длина документа $d$;
    \item $n = \sum_{d, w} n_{dw}$ "--- длина коллекции.
\end{itemize}

Также в тематическом моделировании используются следующие гипотезы и
аксиомы~\cite{all}:
\begin{itemize}
    \item независимость слов от порядка в документе: порядок слов в документе
    не важен;
    \item независимость от порядка документов в коллекции: порядок документов
    в коллекции не важен;
    \item зависимость терма от темы: каждый терм связан с соответствующей темой
    и порождается ей;
    \item гипотеза условной независимости: $p(w|d, t) = p(w|t)$.
\end{itemize}

Вышеперечисленные характеристики, гипотезы и аксиомы составляют основу
тематического моделирования и являются достаточными для построения
тематической модели.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Задача тематического моделирования}
Как уже говорилось ранее, документ порождается следующим образом:
\begin{enumerate}
    \item для каждой позиции в документе генерируется тема $p(t|d)$;
    \item для каждой сгенерированной темы в соответствующей позиции генерируется
    терм $p(w|d, t)$.
\end{enumerate}

\begin{figure}[!ht]
	\centering
	\includegraphics[width=16cm]{./images/2.png}
	\caption{\label{fig:2}%
	Алгоритм формирования документа}
\end{figure}

Тогда вероятность появления слова в документе можно описать по формуле полной
вероятности~\cite{teorver, all}:
\begin{equation}
    p(w|d) = \sum_{t \in T} p(w|d,t)p(t|d) = \sum_{t \in T} p(w|t)p(t|d) 
\end{equation}

Такой алгоритм является прямой задачей порождения текста. Тематическое
моделирование призвано решить обратную задачу:
\begin{enumerate}
    \item для каждого терма $w$ в тексте найти вероятность появления в теме $t$
    (найти $p(w|t) = \phi_{wt}$);
    \item для каждой темы $t$ найти вероятность появления в документе $d$
    (найти $p(t|d) = \theta_{td}$).
\end{enumerate}

Обратную задачу можно представить в виде стохастического матричного
разложения~\ref{fig:1}.

\begin{figure}[!ht]
	\centering
	\includegraphics[width=16cm]{./images/1.png}
	\caption{\label{fig:1}%
	Стохастическое матричное разложение}
\end{figure}

Таким образом, тематическое моделирование ищет величину $p(w|d)$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Решение обратной задачи}
Для решения задачи тематического моделирования необходимо найти величину
$p(w|d)$, сделать это можно с помощью метода максимального правдоподобия.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Лемма о максимизации функции на единичных симплексах} \label{sec:01}
Перед тем как перейти к решению обратной задачи, сформулируем лемму,
которая поможет в этом процессе~\cite{all}.

Введём операцию нормировки вектора:
\begin{equation}
    p_i = \underset{i \in I}(x_i) =
    \frac{\max{x_i, 0}}{\sum_{k \in I} \max{x_k, 0}}
\end{equation}

\textbf{Лемма о максимизации функции на единичных симплексах~\cite{simplex}:}

Пусть функция $f(\Omega)$ непрерывно дифференцируема по набору векторов
$\Omega = (w_i)_{j \in J}, \;\; w_j = (w_{ij})_{i \in I_j}$ различных
размерностей $|I_j|$. Тогда векторы $w_j$ локального экстремума задачи
\begin{equation*}
    \begin{cases}
        f(\Omega) \to \underset{\Omega}{\max} \\
        \sum_{i \in I_j} w_{ij} = 1, \;\; j \in J \\
        w_{ij} \geq 0, \;\; i \in I_j, j \in J
    \end{cases}
\end{equation*}

при условии $1^0: \;\; (\exists i \in I_j) w_{ij}
\frac{\partial f}{\partial w_{ij}} > 0$ удовлетворяют уравнениям
\begin{equation}
    w_{ij} = \underset{i \in I_j}{norm}\left(w_{ij}
    \frac{\partial f}{\partial w_{ij}}\right), \;\; i \in I_j;
\end{equation}

при условии $2^0: \;\; (\forall i \in I_j) w_{ij}
\frac{\partial f}{\partial w_{ij}} \leq 0$ и $(\exists i \in I_j) w_{ij}
\frac{\partial f}{\partial w_{ij}} < 0$ удовлетворяют уравнениям
\begin{equation}
    w_{ij} = \underset{i \in I_j}{norm}\left(-w_{ij}
    \frac{\partial f}{\partial w_{ij}}\right), \;\; i \in I_j;
\end{equation}

в противном случае (условие $3^0$) "--- однородным уравнениям
\begin{equation}
    w_{ij}\frac{\partial f}{\partial w_{ij}} = 0, \;\; i \in I_j.
\end{equation}

Данная лемма служит для оптимизации любых моделей, параметрами которых являются
неотрицательные нормированные векторы.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Сведение обратной задачи к задаче максимизации функционала}
Чтобы вычислить величину $p(w|d)$ воспользуемся принципом максимума
правдоподобия~\cite{teorver}, согласно которому будут подобраны параметры
$\Phi, \Theta$ такие, что $p(w|d)$ примет наибольшее значение.

\begin{equation}
    \prod^n_{i = 1} p(d_i, w_i) = \prod_{d \in D} \prod_{w \in d} p(d, w)^{
        n_{dw}}
\end{equation}

Прологарифмировав правдоподобие, перейдём к задаче максимизации логарифма 
правдоподобия.
\begin{equation}
    \sum_{d \in D} \sum_{w \in d} n_{dw} \ln{p(w|d)
    \underset{const}{\xcancel{p(d)}}} = n_{dw} \to max
\end{equation}

Данная задача эквивалентна задаче максимизации функционала
\begin{equation} \label{eq:01}
    L(\Phi, \Theta) = \sum_{d \in D} \sum_{w \in d} n_{dw} \ln{\sum_{t \in T}
    \phi_{wt} \theta_{td}} \to \underset{\Phi, \Theta}{max}
\end{equation}

при ограничениях неотрицательности и нормировки
\begin{equation} \label{eq:02}
    \phi_{wt} \geq 0; \;\; \sum_{w \in W} \phi_{wt} = 1; \;\;\; \theta_{td}
	\geq 0; \;\; \sum_{t \in T} \theta_{td} = 1
\end{equation}

Таким образом, обратная задача сводится к задаче максимизации
функционала~\cite{ARTM}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Аддитивная регуляризация тематических моделей}
Задача~\ref{eq:01} не соответствует критериям корректно поставленной задачи
по Адамару, поскольку в общем случае она имеет бесконечное множество решений.
Это свидетельствует о необходимости доопределения задачи. 

Для доопределения некорректно поставленных задач применяется регуляризация: к
основному критерию добавляется дополнительный критерий "--- регуляризатор,
который соответствует специфике решаемой задачи. 

Метод ARTM (аддитивная регуляризация тематических моделей~\cite{ARTM})
основывается на максимизации линейной комбинации логарифма правдоподобия и
регуляризаторов $R_i(\Phi, \Theta)$ с неотрицательными коэффициентами
регуляризации $\tau_i, \;\; i = 1, \dots, k$.

Преобразуем задачу к ARTM виду:
\begin{equation}
    \sum_{d \in D} \sum_{w \in d} n_{dw} \ln{\sum_{t \in T}
    \phi_{wt} \theta_{td}} + R(\Phi, \Theta) \to \underset{\Phi, \Theta}{max};
    \;\; R(\Phi, \Theta) = \sum^k_{i = 1} \tau_i R_i(\Phi, \Theta)
\end{equation}

при ограничениях неотрицательности и нормировки~\ref{eq:02}.

Регуляризатор (или набор регуляризаторов) выбирается в соответствии с решаемой
задачей.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{E"=M алгоритм}
Из представленных выше ограничений~\ref{eq:02} следует, что столбцы матриц можно
считать неотрицательными единичными векторами. Таким образом, задача сводится к
максимизации функции на единичных симплексах\cite{all}.

Воспользуемся леммой о максимизации функции на единичных
симплексах~\ref{sec:01} и перепишем задачу.

Пусть функция $R(\Phi, \Theta)$ непрерывно дифференцируема. Тогда точка $(\Phi,
\Theta)$ локального экстремума задачи с ограничениями, удовлетворяет системе
уравнений с вспомогательными переменными $p_{twd} = p(t|d, w)$, если из
решения исключить нулевые столбцы матриц $\Phi$ и $\Theta$:

\begin{equation} \label{eq:03}
    \begin{cases}
        p_{tdw} = \underset{t \in T}{norm}(\phi_{wt}\theta_{td}) \\
        \phi_{wt} = \underset{w \in W}{norm}\left(n_{wt} + \phi_{wt}
        \frac{\partial R}{\partial \phi_{wt}}\right); \\
        \theta_{td} = \underset{t \in T}{norm}\left(n_{td} + \theta_{td}
        \frac{\partial R}{\partial \theta_{td}}\right)
    \end{cases}
\end{equation}

Полученная модель соответствует E"=M алгоритму, где первая строка системы
уравнений соответствует E"=шагу, а вторая и третья строки "--- M"=шагу.

Решив полученную систему уравнений, методом простых итерации получим искомые
матрицы $\Phi$ и $\Theta$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Регуляризаторы в тематическом моделировании}
В этом разделе будут рассмотрены некоторые возможные варианты регуляризаторов.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Дивергенция Кульбака"=Лейблера} \label{sec:02}
Перед тем как перейти к регуляризаторам необходимо ввести меру оценки близости
тем.

Чтобы оценить близость тем можно воспользователься дивергенцией
Кульбака"=Лейблера~\cite{deeplearn} (KL или KL"=дивергенция). KL"=дивергенция
позволяет оценить степень вложенности одного распределения в другое, в случае
тематического моделирования будет оценитьваться вложенность матриц.

Определим KL"=дивергенцию:

Пусть $P = (p_i)^n_{i = 1}$ и $Q = (q_i)^n_{i = 1}$ некоторые распределения.
Тогда дивергенция Кульбака"=Лейблера имеет следующий вид:
\begin{equation}
    KL(P||Q) = KL_i(p_i||q_i) = \sum^n_{i = 1} p_i \ln{\frac{p_i}{q_i}}.
\end{equation}

Свойства KL"=дивергенции:
\begin{enumerate}
    \item $KL(P||Q) \geq 0$;
    \item $KL(P||Q) = 0 \;\; \Leftrightarrow \;\; P = Q$;
    \item Минимизация KL эквивалентна максимизации правдоподобия:
    $$KL(P||Q(\alpha)) = \sum^n_{i = 1} p_i \ln{\frac{p_i}{q_i(\alpha)}} \to
    \underset{\alpha}{\min} \;\; \Leftrightarrow \;\; \sum^n_{i = 1} p_i
    \ln{q_i}(\alpha) \to \underset{\alpha}{\max};$$ \label{it:kl3}
    \item Если $KL(P||Q) < KL(Q||P)$, то $P$ сильнее вложено в $Q$, чем $Q$ в
    $P$.
\end{enumerate}

Теперь можно перейти к рассмотрению регуляризаторов.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Регуляризатор сглаживания}
Сглаживание предполагает сематническое сближение тем, это может быть полезно в
следующих случаях~\cite{reg}:
\begin{enumerate}
    \item Темы могут быть похожи между собой по терминологии, например,
    основы теории вероятностей и линейной алгебры обладают рядом одинаковых
    терминов;
    \item При выделении фоновых тем важно максимально вобрать в них слова,
    следовательно, сглаживание поможет решить эту задачу.
\end{enumerate}

Определим регуляризатор сглаживания:

Пусть распределения $\phi_{wt}$ близки к заданному распределению $\beta_w$
и пусть распределения $\theta_{td}$ близки к заданному распределению $\alpha_t$.
Тогда в форме KL"=дивергеннции~\ref{sec:02} выразим задачу сглаживания:
\begin{equation}
    \sum_{t \in T} KL(\beta_w||\phi_{wt}) \to \underset{\Phi}{\min}; \;\;
    \sum_{d \in D} KL(\alpha_t||\theta_{td}) \to \underset{\Theta}{\min}.
\end{equation}

Согласно свойству~\ref{it:kl3} KL"=дивергенции перейдём к задаче максимизации
правдоподобия:
\begin{equation}
    R(\Phi, \Theta) = \beta_o \sum_{t \in T}\sum_{w \in W} \beta_w
	\ln{\phi_{wt}} + \alpha_0 \sum_{d \in D}\sum_{t \in T}\alpha_t
	\ln{\theta_{td}} \to \max.
\end{equation}

Перепишем EM"=алгоритм~\ref{eq:03} в соответствии с полученной формулой:
\begin{equation} \label{eq:04}
    \begin{cases}
        p_{tdw} = \underset{t \in T}{norm}(\phi_{wt}\theta_{td}) \\
        \phi_{wt} = \underset{w \in W}{norm}\left(n_{wt} + \beta_0\beta_w
        \right); \\
        \theta_{td} = \underset{t \in T}{norm}\left(n_{td} + \alpha_0\alpha_t
        \right)
    \end{cases}
\end{equation}

Таким образом был получен модифицированный EM"=алгоритм соответствующий модели
LDA~\cite{reg}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Регуляризатор разреживания}
Разреживание подразумевает разделение тем и документов, исключая общие слова из
них. Этот тип регуляризации основывается на предположении, что темы и документы
в основном являются специфичными и описываются относительно небольшим набором
терминов, которые не встречаются в других темах.

Определим регуялризатор разреживания~\cite{reg}:

Пусть распределения $\phi_{wt}$ далеки от заданного распределения $\beta_w$
и пусть распределения $\theta_{td}$ далеки от заданного распределения
$\alpha_t$.
Тогда в форме KL"=дивергеннции~\ref{sec:02} выразим задачу сглаживания:
\begin{equation}
    \sum_{t \in T} KL(\beta_w||\phi_{wt}) \to \underset{\Phi}{\max}; \;\;
    \sum_{d \in D} KL(\alpha_t||\theta_{td}) \to \underset{\Theta}{\max}.
\end{equation}

Согласно свойству~\ref{it:kl3} KL"=дивергенции перейдём к задаче максимизации
правдоподобия:
\begin{equation}
    R(\Phi, \Theta) = -\beta_o \sum_{t \in T}\sum_{w \in W} \beta_w
	\ln{\phi_{wt}} - \alpha_0 \sum_{d \in D}\sum_{t \in T}\alpha_t
	\ln{\theta_{td}} \to \max.
\end{equation}

Перепишем EM"=алгоритм~\ref{eq:03} в соответствии с полученной формулой:
\begin{equation} \label{eq:05}
    \begin{cases}
        p_{tdw} = \underset{t \in T}{norm}(\phi_{wt}\theta_{td}) \\
        \phi_{wt} = \underset{w \in W}{norm}\left(n_{wt} - \beta_0\beta_w
        \right); \\
        \theta_{td} = \underset{t \in T}{norm}\left(n_{td} - \alpha_0\alpha_t
        \right)
    \end{cases}
\end{equation}

Таким образом был получен модифицированный EM"=алгоритм, разреживающий
матрицы $\Phi$ и $\Theta$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Регуляризатор декоррелирования тем}
Декоррелятор тем "--- это частный случай разреживания, призванный выделить
для каждой темы лексическое ядро "--- набор термов, отличающий её от других
тем:

Определим регуляризатор декоррелирования~\cite{reg}:

Минимизируем ковариации между вектор"=столбцами $\phi_t$:
\begin{equation}
    R(\Phi) = - \frac{\tau}{2} \sum_{t \in T}\sum_{s \in T \backslash t}
    \sum_{w \in W} \phi_{wt}\phi_{ws} \to max.
\end{equation}

Перепишем EM"=алгоритм~\ref{eq:03} в соответствии с полученной формулой:
\begin{equation} \label{eq:06}
    \begin{cases}
        p_{tdw} = \underset{t \in T}{norm}(\phi_{wt}\theta_{td}) \\
        \phi_{wt} = \underset{w \in W}{norm}\left(n_{wt} - \tau\phi_{wt}
        \sum_{t \in T \backslash t} \phi_{ws}\right); \\
        \theta_{td} = \underset{t \in T}{norm}\left(n_{td} + \theta_{td}
        \frac{\partial R}{\partial \theta_{td}}\right)
    \end{cases}
\end{equation}

Таким образом был получен модифицированный EM"=алгоритм, декоррелирующий
темы.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Оценка качества моделей тематического моделирования}
После построения модели, очевидно, нужно оценить её качество.

Перечислим основные критерии оценки качества тематических
моделей~\cite{all, reg}:

\begin{enumerate}
    \item Внешние критерии (оценка производится экспертами):
    \begin{enumerate}
        \item полнота и точность тематического поиска;
        \item качество ранжирования при тематическом поиске;
        \item качество классификации / категоризации документов;
        \item качество суммаризации / сегментации документов;
        \item экспертные оценки качества тем.
    \end{enumerate}
    \item Внутренние критерии (оценка производится программно):
    \begin{enumerate}
        \item правдоподобие и перплексия;
        \item средняя когерентность (согласованность тем);
        \item разреженность матриц $\Phi$ и $\Theta$;
        \item различность тем;
        \item статический тест условной независимости.
    \end{enumerate}
\end{enumerate}

Поскольку оценка по внешним критериям невозможна в рамках данной работы,
сосредоточимся на внутренних критериях оценки, которые можно вычислять
автоматически.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Правдоподобия и перплексия}
Перплексия основывается на логарифме правдоподобия и является его некоторой
модификацией~\cite{all}.

\begin{equation}
    P(D) = \exp\left(- \frac{1}{n} \sum_{d \in D} \sum_{w \in d} n_{dw}
    \ln{p(w|d)}\right), \;\;\; n = \sum_{d \in D} \sum_{w \in d} n_{dw}
\end{equation}

Не трудно заметить, что при равномерном распределении слов в тексте выполняется
равенство $p(w|d) = \frac{1}{|W|}$. В этом случае значение перплексии равно
мощности словаря $P = |W|$. Это позволяет сделать вывод, что перплексия является
мерой разнообразия и неопределенности слов в тексте: чем меньше значение
перплексии, тем более разнообразны вероятности появления слов.

Таким образом, чем меньше перплексия, тем больше слов с большей вероятностью
$p(w|d)$, которые модель умеет лучше предсказывать, следовательно, чем меньше
перплексия, тем лучше. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Когерентность}
Когерентность является мерой, коррелирующей с экспертной оценкой
интерпретируемости тем~\cite{all}.

Когерентность (согласованность) темы $t$ по $k$ топовым словам:
\begin{equation}
    PNI_t = \frac{2}{k (k - 1)} \sum^{k - 1}_{i = 1} \sum^k_{j = i + 1}
    PMI(w_i, w_j),
\end{equation}
где $w_i$ "--- $i$"=ое слово в порядке убывания $\phi_{wt}$, $PMI(u, v) =
\ln{\frac{|D|N_{uv}}{N_uN_v}}$ "--- поточечная взаимная информация,
$N_{uv}$ "--- число документов, в которых слова $u, v$ хотя бы один раз
встречаются рядом (расстояние опледеляется отдельно), $N_u$ "--- число
документов, в которых $u$ встретился хотя бы один раз.

Гипотезу когерентности можно выразить так~\cite{reg}: когда человек говорит о какой"=либо
теме, то часто употребляет достаточно ограниченный набор слов, относящийся
к этой теме, следовательно, чем чаще будут встречаться вместе слова этой темы,
тем лучше её можно будет интерпретировать.

Сама когерентность берёт самые часто встречающиеся слова из тем, и вычисляет
для каждой пары из них насколько они часто встречаются, соответственно, чем
выше будет значение взаимовстречаемости,
тем лучше~\cite{reg}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Разреженность}
Разреженность "--- доля нулевых элементов в матрицах $\Phi$ и
$\Theta$~\cite{reg}.

Разреженность играет ключевую роль в выявлении различий между темами.
Каждая тема формируется на основе ограниченного набора слов, в то время как
остальные слова должны встречаться реже, что отражается в нулевых элементах
матриц. Оптимальный уровень разреженности должен быть высоким, но не чрезмерным:
в таком случае темы будут четко различимы. Если разреженность слишком низка,
темы могут сливаться, а если слишком высока "--- содержать недостаточное
количество слов для адекватного
представления.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Чистота темы}
Чистота темы:
\begin{equation}
	\sum_{w \in W_t} p(w|t),
\end{equation}

где $W_t$ "--- ядро темы:
$W_t = \{w: p(w|t) > \alpha\}$, где $\alpha$ подбирается по разному,
например $\alpha = 0.25$ или $\alpha = \frac{1}{|W|}$.

Данная характеристика показывает как вероятностно относится ядро темы к фоновым
словам темы, следовательно, чем больше вероятность ядра, тем
лучше~\cite{reg}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Контрастность темы}
Контрастность темы:
\begin{equation}
	\frac{1}{|W_T} \sum_{w \in W_t} p(t|w).
\end{equation}

Данная характеристика показывает насколько часто слова из ядра темы
встречаются в других темах, очевидно, что чем меньше ядро будет встречаться
в других темах, тем лучше~\cite{reg}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Тематическое моделирование новостей}
В данном разделе будет выполнено тематическое моделирование новостей.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Описание данных для моделирования}
Набор данных представляет собой таблицу с новостями, имеющую следующий
вид~\ref{fig:12}.

\begin{figure}[!ht]
	\centering
	\includegraphics[width=15cm]{./images/17.png}
	\caption{\label{fig:12}%
	Пример таблицы данных}
\end{figure}
\newpage

Сами данные были получены с помощью парсинга (библиотеки beuatifulsoap4
и selenium языка python) новостного сайта ВШЭ.

Информация о длинах новостей и их количестве представлена в таблице ниже:

\begin{center}
    \begin{tabular}{l l}
        \textbf{Характеристика} & \textbf{Значение} \\
        всего новостей & 15768 \\
        медианная длина & 29 \\
        средняя длина & 34.6 \\
        std & 12.1 \\
        минимальная длина & 3 \\
        25\% & 26 \\
        50\% & 34 \\
        75\% & 43 \\
        макимальная длина & 87 \\
    \end{tabular}
\end{center}

Информацию о распределении длин документов можно увидеть на следующем
графике~\ref{fig:9}.

\begin{figure}[!ht]
	\centering
	\includegraphics[width=12cm]{./images/14.png}
	\caption{\label{fig:9}%
	Распределение длин новостей}
\end{figure}
\newpage

По нему можно заметить, что в основном длины новостей лежат в диапазоне от
20 до 50 слов.

Иллюстрация закона Ципфа (Самое часто встречающееся слово встречается в два
раза чаще, чем второе по встречаемости слово. Данная зависимость распространяется
на оставшиеся слова.) представлена на следующем графике~\ref{fig:10}.

\begin{figure}[!ht]
	\centering
	\includegraphics[width=12cm]{./images/15.png}
	\caption{\label{fig:10}%
	Закон ципфа}
\end{figure}

По графику можно сказать, что не во всех местах закон Ципфа выполняется,
в некоторых случаях разрыв между частотой встречаемости слов превышает два раза,
что свидетельствует о наличии частого употребления слов общей лексики.

Иллюстрация закона Хипса (распределение количества уникальных слов по длинам
документов) представлена на следующем графике~\ref{fig:11}.

\begin{figure}[!ht]
	\centering
	\includegraphics[width=12cm]{./images/16.png}
	\caption{\label{fig:11}%
	Закон Хипса}
\end{figure}
\newpage

По данному графику можно сказать, что количество уникальных слов прямо
пропорционально зависит от длины документа.

Суммируя информацию выше, можно сказать, что данная коллекция новостей
представляет собой набор недлинных документов (медианная длина новости "--- 29
слов), обладающих высоким процентом слов общей лексики
(сведетельствует о возможной семантической близости новостей), число уникальных
слов в которых прямо пропорционально зависит от длины документа.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Предобработка текстов}
Перед любым моделированием данные нужно подготовить. Ниже приведён стандартный
набор предобработки текстов для тематического моделирования~\cite{predobr}:
\begin{itemize}
    \item токенизация;
    \item перевод текста в нижний регистр;
    \item удаление неалфавитных символов;
    \item удаление стоп слов;
    \item лемматизация;
    \item создание n"=грамм.
\end{itemize}

После выполнения вышеописанных операций можно будет приступать к самому
тематическому моделированию.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Токенизация, перевод в нижний регистр и удаление неалфавитных символов}
Токенизация "--- это разделение текста на составные части "--- токены
(предложения и слова).

Провести токенизацию можно с помощью средств языка python, библиотека nltk.
За токенизацию отвечают команды:
\begin{minted}{python}
    # разделить текст на предложения
    nltk.sent_tokenize(<sentences>)
    # разделить предложение на слова
    nltk.word_tokenize(<sentence>)
\end{minted}

После того как текст поделен на слова, нужно перевести все слова в нижний
регистр, так как семантическое значение слов, чаще всего, не зависит от
регистра. Перевод в нижний регистр можно с помощью стандартных средств языка
python:
\begin{minted}{python}
    # перевести текст в нижний регистр
    <text>.lower()
\end{minted}

После перевода в нижний регистр нужно удалить все семантически незначимые
символы, в данном случае будем рассматривать в качестве таких символов
все символы, не совпадающие с символами русского и английского алфавитов.
Чтобы провести удаление неалфавитных символов достаточно средств языка python:
\begin{minted}{python}
    new_word = ''
    # перебираем символы некоторого слова
    for symbol in word:
        # если символ принадлежит русскому или английскому алфавитам
        if ( symbol >= 'a' and symbol <= 'z'
             or symbol >= 'а' and symbol <= 'я' ):
            # добавляем символ в новое слово
            new_word += symbol
\end{minted}

Таким образом, получим разбитый на слова текст, не содержащий неалфавитных
символов~\cite{videolec4, python-book, predobr}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Удаление стоп"=слов}
Стоп"=слова "--- это слова, которые не несут смысловой нагрузки в рамках,
некоторой темы.

Любой текст содержит большое количество слов общей тематики "--- стоп"=слов.
Такие слова, для улучшения качества модели, можно удалить, так как такие
слова не несут семантической нагрузки, то будут только сбивать модель.

Чтобы удалить стоп"=слова можно воспользоваться библиотки nltk
языка python:
\begin{minted}{python}
    new_words = []
    # перебираем список слов
    for word in words:
        # проверяем какому алфавиту принадлежат символы слова
        if re.match('[а-я]', word):
            # если слово не принадлежит списку стоп слов
            if word not in (stopwords.words('russian')):
                # добавляем слово в новый список слов
                new_words.append(word)
        elif re.match('[a-z]', word):
            # если слово не принадлежит списку стоп слов
            if word not in stopwords.words('english'):
                # добавляем слово в новый список слов
                new_words.append(word)
\end{minted}

Таким образом, получим список слов, в котором будет отсутствовать большинство
стоп"=слов~\cite{videolec4, python-book, predobr}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Лемматизация}
Лемматизация "--- процесс приведения слова к его начальной форме.

Так как семантическое значение слова для темы не зависит от его формы и падежа,
то перед построением модели важно привести все слова в начальную форму, сделать
это можно с помощью библиотек nltk и pymorphy2 языка python:
\begin{minted}{python}
    # создаём лемматизаторы
    lemm_nltk = WordNetLemmatizer()
    lemm_pymorphy2 = pymorphy2.MorphAnalyzer()
    
    new_words = []
    # перебираем список слов
    for word in words:
        # проверяем какому алфавиту принадлежат символы слова
        if re.match('[а-я]', word):
            # лемматизируем слово на русском и добавляем его
            # в новый список слов
            new_words.append(lemm_pymorphy2.parse(word)[0].normal_form)
        elif re.match('[a-z]', word):
            # лемматизируем слово на английском и добавляем его
            # в новый список слов
            new_words.append(lemm_nltk.lemmatize(word))
\end{minted}

Таким образом, получим список слов, приведённых к их начальной
форме~\cite{videolec4, python-book, predobr}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Создание N"=грамм}
N"=грамма "--- это склеивание слов в словосочетание, слов может быть несколько.

Часто слова в теме встречаются в парах или тройках подряд, тогда, если
склеить слова в N"=грамм, то качество и интерпретируемость моделли может
вырасти.

Сделать N"=граммы можно средствами библиотеки nltk языка python:
\begin{minted}{python}
    n_gramms = []
    # перебираем предложения и составляем список n-грамм
    for sentence in sentences:
        # делаем n граммы и добавляем их в список n-грамм
        n_gramms.append(sentence.split(' '), <n>)
\end{minted}

Таким образом, получим список n"=грамм, составленный из начального списка
слов~\cite{videolec4, python-book, predobr}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Создание тематической модели с помощью библиотеки BigARTM}
Блок тематических моделей уже реализован в библиотеке BigARTM, которую
можно использовать на языке python.

Модели BigARTM для своей работы требуют особого типа данных "--- vowpal\_wabbit.
Данный тип данных представляет из себя следующую конструкцию.
\begin{center}
    \begin{tabular}{c c c c c}
        doc\_1 & слово документа 1 & слово документа 1 & \dots & слово документа 1 \\
        doc\_2 & слово документа 2 & слово документа 2 & \dots & слово документа 2 \\
        \dots & \dots & \dots & \dots & \dots \\
        doc\_n & слово документа n & слово документа n & \dots & слово документа n \\
    \end{tabular}
\end{center}

Преобразовать excel таблицу с новостями к данному формату можно с помощью
стандартных средств языка python и библиотеки pandas:
\begin{minted}{python}
    # считываем excel таблицу в pandas DataFrame
    data = pd.read_excel('news.xlsx')
    # открываем файл для записи vowpal_wabbit файла
    f = open(<path>, 'w')
    # проходимся по строкам DataFrame
    for string in range(data.shape[0]):
        # записываем отдельную новость в файл как отдельный документ
        f.write( 'doc_{0}'.format(string)
               + data.loc[string, 'title']
               + ' '
               + data.loc[string, 'content']
               + '\n')
    # после записи закрываем файл
    f.close()
\end{minted}

Чтобы передать данные из vowpal\_wabbit файла в модель необходимо создать
батчи, они удобно будут постепенно загружаться в оперативную память по мере
необходимости и передаваться на моделирование, кроме того батчи автоматически
вычисляют для себя словарь, который также необходим при создании модели. Создать
батчи можно следующим образом:
\begin{minted}{python}
    # data_path - путь к vowpal_wabbit файлу
    # data_format - формат загружаемого файла - vowpal_wabbit
    # batch_size - количество документов в одном батче
    # target_folder - папка, в которую батчи сохраняются
    bv = artm.BatchVectorizer( data_path = 'vw.txt',
                               data_format = 'vowpal_wabbit',
                               batch_size=3000,
                               target_folder='batches' )
\end{minted}

Наконец, можно создать саму модель, делается это следующим образом:
\begin{minted}{python}
    # num_topics - количество тем
    # num_document_passes - количество проходов
    # по каждому документу (новости)
    # dictionary - словарь
    # class_ids - веса для модальностей
    # создание модели
    model = artm.ARTM( num_topics=7,
                       num_document_passes=3,
                       dictionary=bv.dictionary,
                       class_ids={'@default_class': 1.0})
    # добавление метрик
    model.scores.add( artm.PerplexityScore( name='perplexity',
                      dictionary=bv.dictionary ) )
    # сохранения топа слов для каждой темы
    model.scores.add(artm.TopTokensScore(name='top-tokens', num_tokens=10))
    # добавление регуляризаторов, например, декоррелятора
    # tau - коэффициент регуляризации
    model.regularizers.add( artm.DecorrelatorPhiRegularizer( name='decorrelator',
                                                             tau=2e7 ) )
\end{minted}

Метрик качества, а также регуляризаторов можно добавить сразу несколько.

После создания модели нужно провести моделирование, сделать это можно следующим
образом:
\begin{minted}{python}
    for _ in range(<num_passes>):
        model.fit_offline(bv, num_collection_passes=1)
\end{minted}

Чтобы оценить модель можно запросить значение метрик и список слов для тем:
\begin{minted}{python}
    # запрашиваем последнее значение перплексии
    perplexity = model.score_tracker['perplexity'].last_value
    # запрашиваем массив самых популярных слов для каждой темы
    top_tokens = model.score_tracker['top-tokens'].last_value
\end{minted}

Таким образом, получим построенную тематическую
модель~\cite{videolec4, python-book, bigartm-book, bigartm, pandas}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{PLSA (модель без регуляризаторов)}
Модели PLSA соответствует EM"=алгоритм без регуляризаторов~\ref{eq:03}.
Данную модель можно создать средствами библиотеки BigARTM следующим образом:
\begin{minted}{python}
    model = artm.ARTM( num_topics=param1,
                       num_document_passes=param3,
                       dictionary=bv.dictionary,
                       class_ids={'@default_class': 1.0} )

    model.scores.add( artm.PerplexityScore( name='perplexity',
                                            dictionary=bv.dictionary ) )
    model.scores.add(artm.SparsityPhiScore(name='sparsity_phi_score'))
    model.scores.add(artm.SparsityThetaScore(name='sparsity_theta_score'))
    model.scores.add(artm.TopTokensScore(name='top-tokens', num_tokens=10))
\end{minted}

Для оценки качества модели выбраны такие характеристика как перплексия и
разреженность (по матрицам $\Phi$ и $\Theta$).

На место параметров модели (param1, param3) в функции создания и вычисления
(построение будет происходить на простых словах, биграммах и триграммах),
которую можно увидеть в приложениях, будут подставляться значения из некоторого
набора, затем модель будет прогнана param2 раз по коллекции. По окончании
результаты будут собраны в соответствующую таблицу~\ref{sec:02}.

Наилучшие значения перплексии достигаются при 8 темах, 24 проходах по коллекции
и 4 проходах по каждому документу. Скорее всего модель без регуляризаторов
не сильно подходит для тематического моделирования новостей, так как темы,
скорее всего семантически близки друг к другу, поэтому их стоит разреживать.

Вариант с N"=граммами не прошёл, скорее всего, из"=за топорности их создания
библиотекой nltk~\cite{All1, all, python-book, bigartm-book, bigartm}.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{LDA (модель с регуляризатором сглаживания)}
Модели LDA соответствует EM"=алгоритм с регуляризатором сглаживания~\ref{eq:04}.
Создать модель можно следующим образом:
\begin{minted}{python}
    model = artm.ARTM( num_topics=param1,
                       num_document_passes=param3,
                       dictionary=bv.dictionary,
                       class_ids={'@default_class': 1.0} )
    model.regularizers.add( artm.SmoothSparsePhiRegularizer( name='smooth',
                                                             tau=tau ) )

    model.scores.add( artm.PerplexityScore( name='perplexity',
                                            dictionary=bv.dictionary ) )
    model.scores.add(artm.SparsityPhiScore(name='sparsity_phi_score'))
    model.scores.add(artm.SparsityThetaScore(name='sparsity_theta_score'))
    model.scores.add(artm.TopTokensScore(name='top-tokens', num_tokens=10))
\end{minted}

Для оценки качества модели выбраны такие же характеристки как и у модели PLSA.

На место параметров модели (param1, param3, tau > 0) в функции создания и вычисления,
которую можно увидеть в приложениях, будут подставляться значения из некоторого
набора, затем модель будет прогнана param2 раз по коллекции. По окончании
результаты будут собраны в соответствующую таблицу~\ref{sec:03}.

Модель LDA, ожидаемо, показывает не лучшие результаты в виду особенностей
коллекции новостей (семантическая близость новстей не нуждается в регуляризаторе
сглаживания).~\cite{All1, all, python-book, bigartm-book, bigartm}.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Модель с регуляризатором разреживания}
В данном случае модели соответствует EM"=алгоритм с регуляризатором
разреживания~\ref{eq:05}. Создать модель можно следующим образом:
\begin{minted}{python}
    model = artm.ARTM( num_topics=param1,
                       num_document_passes=param3,
                       dictionary=bv.dictionary,
                       class_ids={'@default_class': 1.0} )
    model.regularizers.add( artm.SmoothSparsePhiRegularizer( name='smooth',
                                                             tau=tau ) )

    model.scores.add( artm.PerplexityScore( name='perplexity',
                                            dictionary=bv.dictionary ) )
    model.scores.add(artm.SparsityPhiScore(name='sparsity_phi_score'))
    model.scores.add(artm.SparsityThetaScore(name='sparsity_theta_score'))
    model.scores.add(artm.TopTokensScore(name='top-tokens', num_tokens=10))
\end{minted}

Характеристики для оценки качества используются всё теже.

На место параметров модели (param1, param3, tau < 0) в функции создания и вычисления,
которую можно увидеть в приложениях, будут подставляться значения из некоторого
набора, затем модель будет прогнана param2 раз по коллекции. По окончании
результаты будут собраны в соответствующую таблицу~\ref{sec:04}.

Модель с регуляризатором разреживания показывает второй по качеству результат
по перплексии, лучшее значение достигается при 7 темах, 3 проходах по каждому из
документов и 15 проходах по всей коллекции.

Несмотря на лучшие значения перплексии при использовании N"=граммов, их
не стоит использовать из"=за слишком большой разреженности,
так как в данном случае размер ядер будет очень
маленьким~\cite{All1, all, python-book, bigartm-book, bigartm}.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Модель с регуляризатором декоррелирования}
В данном случае модели соответствует EM"=алгоритм с регуляризатором
декоррелирования~\ref{eq:06}. Создать модель можно следующим образом:
\begin{minted}{python}
    model = artm.ARTM( num_topics=param1,
                       num_document_passes=param3,
                       dictionary=bv.dictionary,
                       class_ids={'@default_class': 1.0} )
    model.regularizers.add( artm.DecorrelatorPhiRegularizer( name='decorrelator',
                                                             tau=tau ) )

    model.scores.add( artm.PerplexityScore( name='perplexity',
                                            dictionary=bv.dictionary ) )
    model.scores.add(artm.SparsityPhiScore(name='sparsity_phi_score'))
    model.scores.add(artm.SparsityThetaScore(name='sparsity_theta_score'))
    model.scores.add(artm.TopTokensScore(name='top-tokens', num_tokens=10))
\end{minted}

Характеристики для оценки качества используются всё теже.

На место параметров модели (param1, param3, tau) в функции создания и вычисления,
которую можно увидеть в приложениях, будут подставляться значения из некоторого
набора, затем модель будет прогнана param2 раз по коллекции. По окончании
результаты будут собраны в соответствующую таблицу~\ref{sec:05}.

Модель с регуляризатором декорреляции дала наилучший результат среди моделей,
обусловлено это особенностями данных (семантическая близость тем). Лучшее
значение достигается при 8 темах, 24 проходах по всей коллекции и 7 проходах
по каждому документу~\cite{All1, all, python-book, bigartm-book, bigartm}.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Выбор лучшей модели}
Выберем по одной модели из каждого класса, обладающей наибольшим значением
перплексии в своём классе, и повторно построим их.

Значения перплексии и разреженности матриц $\phi$ и $\theta$ моделей
представлены в таблице ниже:

\begin{center}
    \begin{tabular}{c c c c}
        Тип модели & Переплексия & $\Phi$ & $\Theta$ \\
        PLSA & 1329 & 57\% & 0\% \\
        LDA & 1552 & 0\% & 0\% \\
        Sparsity & 1071 & 74\% & 28\% \\
        Decorrelator & 838 & 72\% & 27\% \\
    \end{tabular}
\end{center}

По параметру перплексии лучшей оказалась модель с регуляризатором
декоррелирования, её значения разреженности матриц $\Phi$ и $\Theta$ находятя
в пределах нормы. Однако значения перплексии недостаточно для определения
лучшей модели для задачи моделирования новостных данных, необходимо ещё
посмотреть ядра тем, полученные моделями.

Приведём слова ядер тем для каждой из моделей:

\textbf{PLSA}

\begin{figure}[!ht]
	\centering
	\includegraphics[width=18cm]{./images/10.png}
	\caption{\label{fig:5}%
	Ядра тем модели PLSA}
\end{figure}
\newpage

\textbf{LDA}

\begin{figure}[!ht]
	\centering
	\includegraphics[width=18cm]{./images/11.png}
	\caption{\label{fig:6}%
	Ядра тем модели LDA}
\end{figure}

\textbf{Модель с регуляризатором разреживания}

\begin{figure}[!ht]
	\centering
	\includegraphics[width=18cm]{./images/12.png}
	\caption{\label{fig:7}%
	Ядра тем модели с регуляризатором разреживания}
\end{figure}

\textbf{Модель с регуляризатором декоррелирования}

\begin{figure}[!ht]
	\centering
	\includegraphics[width=18cm]{./images/13.png}
	\caption{\label{fig:8}%
	Ядра тем модели с регуляризатором декоррелирования}
\end{figure}
\newpage

Можно заметить, что ядра тем модели с регуляризатором декоррелирования обладают
лучшей интерпретируемостью, кроме того темы обладают лучшей различимостью по
сравнению с другими моделями.

Таким образом, исходя из значений перплексии, интепретируемости и различимости
тем, можно сделать вывод, что модель с регуляризатором декоррелирования подходит
лучше, чем отстальные три вида моделей, для моделирования новостных данных.

% Раздел "Заключение"
\conclusion
В ходе данной работы были решены следующие задачи:
\begin{enumerate}
    \item рассмотрены теоретические основы тематического моделирования (задача
    тематического моделирования, EM"=алгоритм, аддтивная регуляризация тематических
    моделей, виды регуляризаторов);
    \item изучены набор базовых методов предобработки данных (токенизация,
    лемматизация, удаление стоп"=слов);
    \item построены тематичесские модели четырёх видов с разными параметрами с
    помощью библиотеку BigARTM;
    \item проведён сравнительный анализ полученных моделей, в результате
    которого определён наиболее подходящих тип модели для тематического
    моделирования новостных данных "--- модель с регуляризатором
    декоррелирования.
\end{enumerate}

Таким образом, были решены все поставленные задачи и достигнута цель работы.
%Библиографический список, составленный вручную, без использования BibTeX
%
%\begin{thebibliography}{99}
%  \bibitem{Ione} Источник 1.
%  \bibitem{Itwo} Источник 2
%\end{thebibliography}

%Библиографический список, составленный с помощью BibTeX
%
\bibliographystyle{gost780uv}
\bibliography{thesis}

% Окончание основного документа и начало приложений
% Каждая последующая секция документа будет являться приложением
\appendix

\section{Код программы подготовки данных}
\inputminted[fontsize=\small,encoding=utf8,outencoding=cp1251,breaklines=true,breakafter={.,\space}]{python3}{code/prepare_data.py}

\section{Код программы PLSA модели}
\inputminted[fontsize=\small,encoding=utf8,outencoding=cp1251,breaklines=true,breakafter={.,\space}]{python3}{code/PLSA.py}

\section{Код программы LDA модели}
\inputminted[fontsize=\small,encoding=utf8,outencoding=cp1251,breaklines=true,breakafter={.,\space}]{python3}{code/LDA.py}

\section{Код программы модели с регуляризатором разреживания}
\inputminted[fontsize=\small,encoding=utf8,outencoding=cp1251,breaklines=true,breakafter={.,\space}]{python3}{code/SPARSE.py}

\section{Код программы модели с регуляризатором декоррелирования}
\inputminted[fontsize=\small,encoding=utf8,outencoding=cp1251,breaklines=true,breakafter={.,\space}]{python3}{code/DECOR.py}

\section{Ссылка на ноутбук с программой}
\href{https://colab.research.google.com/drive/1DdeV_w03YHjKFUvIHHEFLzuy3p_4R8eA?usp=sharing}{topic\_modeling.ipynb}

\section{Результаты построения модели PLSA} \label{sec:03}

\begin{figure}[!ht]
	\centering
	\includegraphics[width=17cm]{./images/3.png}

    \includegraphics[width=17cm]{./images/4.png}
	\caption{\label{fig:3}%
	Результат работы модели PLSA}
\end{figure}
\newpage

\section{Результаты построения модели LDA} \label{sec:04}

\begin{figure}[!ht]
	\centering
	\includegraphics[width=18cm]{./images/5.png}
	\caption{\label{fig:4}%
	Результат работы модели LDA}
\end{figure}
\newpage

\section{Результаты построения модели с регуляризатором разреживания} \label{sec:05}

\begin{figure}[!ht]
	\centering
	\includegraphics[width=18cm]{./images/6.png}

    \includegraphics[width=18cm]{./images/7.png}
	\caption{\label{fig:4}%
	Результат работы модели с регуляризатором разреживания}
\end{figure}
\newpage

\section{Результаты построения модели с регуляризатором декоррелирования} \label{sec:06}

\begin{figure}[!ht]
	\centering
	\includegraphics[width=18cm]{./images/8.png}

    \includegraphics[width=18cm]{./images/9.png}
	\caption{\label{fig:4}%
	Результат работы модели с регуляризатором сглаживания}
\end{figure}
\newpage

\end{document}